---
title: "Exercise 3"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
  - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
  - \usepackage{booktabs}
output:
  pdf_document: default
  github_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
urlcolor: blue
bibliography: ref.bib
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(
  echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE, strip.white = TRUE,
  prompt = FALSE, cache = TRUE, size = "scriptsize", fig.width = 6, fig.height = 4
)
knitr::opts_knit$set(root.dir = "./Additional files")  # Changed working directory
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Poisson}{\operatorname{Poisson}}
\newcommand{\Exp}{\operatorname{Exponential}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\Betadist}{\operatorname{Beta}}
\newcommand{\Gammadist}{\operatorname{Gamma}}
\newcommand{\InvGamma}{\operatorname{Inv-Gamma}}
\newcommand{\Normal}{\operatorname{Normal}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bfzero}{\mathbf{0}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfeta}{\boldsymbol{\eta}}
\newcommand{\bfbeta}{\boldsymbol{\beta}}
\newcommand{\dtheta}{\, \mathrm{d} \theta}
\newcommand{\dx}{\, \mathrm{d} x}
\newcommand{\dy}{\, \mathrm{d} y}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}  <!-- vector -->
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}  <!-- matrix -->



```{r libraries and help files, eval = TRUE, echo = FALSE}
library(tidyverse)  # Collection of R packages designed for data science
library(matrixStats)  # For row- and column-wise variance and mean of matrices


# Extract pre-programmed R-code
source("probAdata.R")  # Data
source("probAhelp.R")  # Code
```

# Problem A: Comparing AR(2) parameter estimators using resampling of residuals

We consider here the $\mathrm{AR}(2)$ model specified by the relation
$$
  x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t,
$$
where $e_t$ are i.i.d. random variables with zero mean and constant variance, for $t=3, \dots, T$. In our case we analyze the data in `data3A$x`, which is of length $T=100$, and has the form as in Figure \ref{fig:time_series}.
```{r Time series, fig.width = 5, fig.height = 3, fig.cap="\\label{fig:time_series}Time seres data."}

x <- data3A$x
TT <- length(x)
plot(1:TT, x, xlab = "t", ylab = "x", type = "l")

```

The minimizers $\hat{\bfbeta}_\mathrm{LS}$ and $\hat{\bfbeta}_\mathrm{LA}$ are obtained by minimizing the sum of squared residuals (LS) and the sum of absolute residuals (LA), respectively. 
That is, we minimize
$$
  Q_\mathrm{LS}(\bfx) = \sum_{t=3}^T (x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2})^2
  \quad \text{and} \quad
  Q_\mathrm{LA}(\bfx) = \sum_{t=3}^T |x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2}|,
$$
with respect to $\bfbeta = \begin{bmatrix} \beta_1 & \beta_2 \end{bmatrix}^\top$, respectively. We can then define the estimated residuals $\hat{e}_t = x_t - \hat{\beta}_1 x_{t-1} + \hat{\beta}_2 x_{t-2}$, for $t=3,\dots,T$, and if we let $\bar{e}$ be the mean of these, $\hat{\varepsilon}_t = \hat{e}_t - \bar e$ has mean zero.

## Subproblem 1.
In the following code block we generate $B=2000$ bootstrap samples of the residuals, containing $T$ elements randomly sampled from the estimated residuals with replacement. From the sampled residuals the $\mathrm{AR}(2)$ sequence is resampled for each $b = 1, \dots, B$, where the initial values $x_\tau$ and $x_{\tau+1}$ are chosen randomly for each new process.

```{r AR(2)}

set.seed(269)

# Estimates for the betas:
betas <- ARp.beta.est(x, 2)
# Corresponding residuals:
res_LS <- ARp.resid(x, betas$LS)
res_LA <- ARp.resid(x, betas$LA)

# Bootstrap:
B <- 2000   # Number of bootstrap samples
n <- length(res_LS)   # = length(res_LA) is the size of resampling
# Initialize to store samples:
boot_beta_LS <- matrix(NA, nrow = 2, ncol = B)
boot_beta_LA <- matrix(NA, nrow = 2, ncol = B)

for(b in 1:B) {
  # Sample the residuals:
  sample_LS <- sample(res_LS, n, replace = TRUE)
  sample_LA <- sample(res_LA, n, replace = TRUE)
  # Calculate AR(2) sequence and betas:
  x_LS <- ARp.filter(x[rep(sample(TT-1, 1), 2) + c(0, 1)], betas$LS, sample_LS)
  x_LA <- ARp.filter(x[rep(sample(TT-1, 1), 2) + c(0, 1)], betas$LA, sample_LA)
  beta_boot_LS <- ARp.beta.est(x_LS, 2)$LS
  beta_boot_LA <- ARp.beta.est(x_LA, 2)$LA
  # Append betas to the bootstrap matrices:
  boot_beta_LS[, b] <- beta_boot_LS
  boot_beta_LA[, b] <- beta_boot_LA
}

```

We can then find the estimated variances of $\hat{\bfbeta}_\mathrm{LS}$ and $\hat{\bfbeta}_\mathrm{LA}$, and also their bias. The results are shown in the following code block.

```{r AR(2) results}

# Estimated variances of beta1 and beta2 for LS and LA:
rowVars(boot_beta_LS)
rowVars(boot_beta_LA)

# Estimated bias of beta1 and beta2 for LS and LA:
rowMeans(boot_beta_LS) - betas$LS
rowMeans(boot_beta_LA) - betas$LA

```

From this we see that the estimated variance of $\hat{\bfbeta}_\mathrm{LA}$ is smaller than that of $\hat{\bfbeta}_\mathrm{LS}$, and this is also true for the estimated bias. This suggests that the LS estimator is not optimal for the $\mathrm{AR}(2)$ process.



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.
In this part we want to compute a 95% prediction interval for $x_{101}$ based on the LS and the LA estimators, using the bootstrapped time series and parameter estimates obtained earlier. For this we use `boot_res_LS` and `boot_res_LA`, as we found in the last part. Using this we can estimate
$$
  x_{101} = \hat{\beta}_1 x_{100} + \hat{\beta}_2 x_{99} + \hat{\varepsilon}_{101},
$$
where $\hat{\bfbeta}$ is either $\hat{\bfbeta}_\mathrm{LS}$ or $\hat{\bfbeta}_\mathrm{LA}$, and $\hat{\varepsilon}_{101}$ is sampled at random from the residual sample. We can use the residuals as samples for $\hat{\varepsilon}_{101}$ because they are assumed to be independent identically distributed. We do this in the following code block for the LS and the LA estimators.

```{r AR(2) prediction interval}

set.seed(26)

# Finding x_101 for the two estimators:
x_101_LS <- t(boot_beta_LS[, sample(B, replace = TRUE)]) %*% x[TT - 0:1] +
  sample(res_LS, B, replace = TRUE)
x_101_LA <- t(boot_beta_LA[, sample(B, replace = TRUE)]) %*% x[TT - 0:1] +
  sample(res_LA, B, replace = TRUE)

# The prediction intervals for x_101 for LS and LA:
pred_int <- rbind(
  LS = quantile(x_101_LS, c(0.025, 0.975)),
  LA = quantile(x_101_LA, c(0.025, 0.975))
  )

pred_int
```

We see that the 95% prediction interval for $x_{101}$ using the LS estimator is `r round(quantile(x_101_LS, 0.975) - quantile(x_101_LS, 0.025), 2)`, while for the LA estimator it is `r round(quantile(x_101_LA, 0.975) - quantile(x_101_LA, 0.025), 2)`. That is, they are almost equally wide. The prediction interval is also shown in Figure \ref{fig:prediction_intervals}. We conclude that both models perform equally well on predicting the next data point, despite the fact that $\widehat{\Var}[\hat{\vect\beta}_\mathrm{LA}] < \widehat{\Var}[\hat{\vect\beta}_\mathrm{LS}]$.

```{r Time series pred, fig.cap="\\label{fig:prediction_intervals}Time seres data with the 95% prediction intervals for $x_{101}$ using the LS estimator in red and using the LA estimator in blue."}

plot(1:TT, x, xlab = "t", ylab = "x", type = "l")
lines(c(TT-1, TT+3), rep(pred_int["LS", 1], 2), type = "l", col = 2, lwd = 2)
lines(c(TT-1, TT+3), rep(pred_int["LS", 2], 2), type = "l", col = 2, lwd = 2)
lines(c(TT-1, TT+3), rep(pred_int["LA", 1], 2), type = "l", col = 4, lty = 2, lwd = 1.5)
lines(c(TT-1, TT+3), rep(pred_int["LA", 2], 2), type = "l", col = 4, lty = 2, lwd = 1.5)
legend("topleft", legend = c("LS", "LA"), col = c(2, 4), lty = c(1, 2), lwd = 1.5)

```



[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem B: Permutation test

[Bilirubin](http://en.wikipedia.org/wiki/Bilirubin) is a breakdown product of hemoglobin, which is a principal component of red blood cells. If the liver has suffered degeneration,  the decomposition of hemoglobin is elevated, or the gall bladder has been destroyed, large amounts of bilirubin can accumulate in the blood, leading to jaundice, which is a yellowish or greenish pigmentation in the skin.

We will look at a data set taken from JÃ¸rgensen [-@jorgensen1993theory]. It contains measurements of the concentration of bilirubin (mg/dL) in blood samples taken from an three young men, shown in Table \ref{tab:bilirubin}.

```{r bilirubin, echo = FALSE}
bilirubin <- read.table("bilirubin.txt", header = TRUE)
bilirubin %>%
  group_by(pers) %>% 
  mutate(row = row_number()) %>% 
  pivot_wider(names_from = pers, values_from = meas) %>% 
  select(-row) %>%
  set_names(1:3) %>% 
  t() %>% 
  kableExtra::kbl(booktabs = TRUE, format = "latex", row.names = TRUE,
                  caption = "The measurements of bilirubin in three individuals.") %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::add_header_above(c("Individual", "Concentration (mg/dL)" = 11))
```

We will use the $F$-statistic perform a permutation test.

## Subproblem 1.

We set up the following regression model
\begin{equation}\label{eq:B-lm}
\log Y_{ij} = \beta_i + \epsilon_{ij},\quad \text{with}\quad i=1,2,3,\quad \text{and} \quad j = 1,\ldots, n_i,
\end{equation}
where $n_i$ are the number of measurements from individual $i$, and $\epsilon_{ij}\sim\Normal(0,\sigma^2)$.
We want to test if the concentration of bilirubin of the three young men are significantly different, so we set up the following hypothesis test
$$
  H_0 : \beta_1=\beta_2=\beta_3\quad\text{against}\quad H_1 : \exists\, i,j \in\{1,2,3\} : \beta_i\neq\beta_j.
$$

A boxplot of the log measurements for each individual is shown in Figure \ref{fig:B-bilirubin-box}, where we see that individual 1 and individual 2 have similar median measurements, albeit individual 2 having a smaller spread. Individual 3 stands out from the two others.

```{r bilirubin fit, fig.width=4, fig.height=3, fig.align='center', fig.cap="\\label{fig:B-bilirubin-box}Box plot showing the log concentration of biliburin from the three young men."}
bilirubin %>% 
  ggplot(aes(pers, log(meas))) +
  geom_boxplot() +
  theme_minimal()

(lm_fit_summary <- summary(lm(log(meas) ~ pers, bilirubin)))
Fval <- lm_fit_summary$fstatistic[1]
```

The summary output shows the $F$-statistic on 2 and 26 degrees of freedom with the respective $p$-value of 0.03946. With a significance level of $5\%$, we reject the null hypothesis and say there is evidence of the individuals having different concentrations of bilirubin.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.

We create a permutation test function, where we shuffle the individual labels to the data and re-fit a linear model according to \eqref{eq:B-lm} and return the $F$-statistic.

```{r perm}
# Generates permutation of the data, fitting a linear model and returning the F statistic
# data: dataframe consisting of two variables, response and covariate
permTest <- function(data) {
  # gives formula log(y) ~ x, where y is reshuffled
  perm_formula <- update(formula(data), sample(log(.)) ~ .)
  # returns F statistic
  summary(lm(perm_formula, data))$fstatistic[1]
}
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.
We now perform the permutation test with a sample size of 999.
A histogram of the $F$-statistics is shown in Figure \ref{fig:B-F-hist}. We see it fits well under the curve of the theoretical density of the $F$ distribution.

```{r testing perm, fig.width=6, fig.height=3, fig.align='center', fig.cap="\\label{fig:B-F-hist}Normalized histogram of F-statistic from 999 permutations of the \\texttt{bilirubin.txt} data together with the theoretical $F$ distribution on 2 and 26 degrees of freedom. The vertical line is the $F$-statistic of the original model."}
set.seed(42)
Fvals <- replicate(n = 999, permTest(bilirubin))

tibble(Fvals = Fvals, Fval = Fval) %>% 
  ggplot(aes(Fvals)) +
  geom_histogram(
    mapping = aes(y = after_stat(density)),
    breaks  = seq(0, max(Fvals), by=0.2)
    ) +
  geom_function(
    mapping = aes(color = "Theoretical density"),
    fun     = df,
    n       = 1001,
    args    = list(df1 = 2, df2 = 26)
  ) +
  geom_vline(aes(xintercept = Fval, color = "F-value")) +
  theme_minimal()
```

The calculated $p$-value from the permutation test is given below.
```{r testing perm p-val}
p_val <- mean(Fvals > Fval)
print(sprintf("p-value =  %.5f", p_val))
```

This value corresponds well with the $p$-value from the summary output of the original model. Again, with a significance level of $5\%$ we reject the null hypothesis and say there is evidence of the three young men having different levels of bilirubin.

[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem C: The EM-algorithm and bootstrapping

Let $x_1,\ldots, x_n$ and $y_1,\ldots, y_n$ be independent random variables, with $x_i\sim\Exp(\lambda_0)$ and $y_i\sim\Exp(\lambda_1)$, for $i=1,\ldots n$.
Assume that we do not observe $(x_i, y_i), i=1,\ldots,n$, directly.
Instead, we observe
\begin{equation}\label{eq:c-observed}
  \begin{split}
    z_i &= \max(x_i, y_i),\quad \text{for}\quad i=1,\ldots, n, \\
    u_i &= I(x_i \geq y_i),\quad \text{for}\quad i=1,\ldots, n,
  \end{split}
\end{equation}

where $I(\cdot)\in\{0, 1\}$ is the indicator function. A histogram of the observed data is shown in Figure \ref{fig:em-observed}.

Based on the observed values $(z_i, u_i), i=1,\ldots,n$, we will use the EM-algorithm to find the MLE for $(\lambda_0, \lambda_1)$.

```{r em-observed, echo = FALSE, fig.align='center', fig.width=5, fig.height=3, fig.cap="The observed data"}
## Read observed data
z <- scan("z.txt")  # max(x, y)
u <- scan("u.txt")  # I(x >= y)

tibble(z = z, u = factor(u)) %>% ggplot(aes(x = z, fill = u)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 50) +
  labs(fill = expression(u==I(x >= y)), x = "z = max(x, y)") +
  theme_minimal()
```


## Subproblem 1.
Let $\vect x = (x_1,\ldots,x_n), \vect y = (y_1,\ldots, y_n), \vect z = (z_1, \ldots, z_n)$ and $\vect u = (u_1,\ldots, u_n)$.
For the E-step we compute the expectation of the joint log likelihood for the complete data conditioned on the observed data.
$$
\begin{split}
 Q(\lambda_0, \lambda_1\mid \lambda_0^{(t)},\lambda_1^{(t)}) &= \E\left[\log \mathcal L(\lambda_0,\lambda_1\mid \vect X, \vect Y) \mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\
 &= \E\left[\log f(\vect X, \vect Y\mid \lambda_0, \lambda_1)\mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\
 &= \E\left[\log\left(\prod_{i=1}^n f_X(X_i\mid\lambda_0)\cdot f_Y(Y_i\mid\lambda_1)\right)\mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\
 &= \E\left[\sum_{i=1}^n\log\left(\lambda_0\e^{-\lambda_0X_i}\cdot\lambda_1\e^{-\lambda_1Y_i}\right)\mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\
 &= \E\left[\sum_{i=1}^n\left(\log \lambda_0 + \log \lambda_1 - \lambda_0X_i - \lambda_1Y_i\right)\mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\\\
 &= n(\log\lambda_0 + \log \lambda_1) \\
 &\quad -\lambda_0\sum_{i=1}^n\E\left[X_i\mid\vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)}\right] \\
 &\quad - \lambda_1\sum_{i=1}^n\E\left[Y_i\mid\vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)}\right].
\end{split}
$$
To evaluate the expectations in the last equality, we first do some simplifications.
For $X_i,i=1,\ldots, n$, we have the following:
\begin{equation}\label{eq:c-e-x}
\begin{split}
  \E\left[X_i\mid \vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)} \right]
  &=\E\left[X_i\mid \max(X_i, Y_i) = z_i, I(X_i\geq Y_i) = u_i, \lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &=u_i\E\left[X_i\mid\max(X_i,Y_i) = z_i, X_i\geq Y_i,\lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &\quad + (1 - u_i)\E\left[X_i\mid\max(X_i,Y_i) = z_i, X_i< Y_i,\lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &= u_i\E\left[X_i\mid X_i = z_i\right] + (1 - u_i)\E\left[X_i\mid X_i < z_i,\lambda_0 = \lambda_0^{(t)}\right] \\
  &=u_iz_i + (1 - u_i)\E\left[X_i\mid X_i < z_i,\lambda_0 = \lambda_0^{(t)}\right].
\end{split}
\end{equation}
The expectation in the last equality is found by first computing the conditional cdf of $[X\mid X < z, \lambda]$, omitting the subscripts for computational convenience, as we will use the result for $Y_i,i=1,\ldots, n$ as well.

$$
\begin{split}
  F(x\mid X < z,\lambda) &= \Pr(X < x\mid X < z,\lambda) \\
  &= \frac{\Pr(X < x, X < z\mid\lambda)}{\Pr(X< z\mid \lambda)} \\
  &= \frac{F_X(\min(x, z)\mid\lambda)}{F_X(z\mid \lambda)},
\end{split}
$$
where $F_X(\cdot\mid\lambda)\sim\Exp(\lambda)$.
The conditional pdf is then given by
$$
f(x\mid X< z,\lambda) = \frac{d}{dx}\frac{F_X(\min(x, z)\mid\lambda)}{F_X(z\mid \lambda)} = \frac{f_X(x\mid \lambda)}{F_X(z\mid \lambda)},\quad 0<x<z,
$$
giving the conditional expectation
$$
\begin{split}
  \E\left[X\mid X < z,\lambda\right] &= \int_0^z x f(x\mid X< z,\lambda)\,\mathrm{d}x \\
  &= \int_0^z x \frac{f_X(x\mid \lambda)}{F_X(z\mid \lambda)}\,\mathrm{d}x \\
  &= \int_0^z x\frac{\lambda\e^{-\lambda x}}{1 - \e^{-\lambda z}}\,\mathrm d x \\
  &= \frac{1}{1-\e^{-\lambda x}}\int_0^z \lambda x\e^{-\lambda x}\,\mathrm d x \\
  &= \frac{1}{1-\e^{-\lambda x}}\left(\frac{1}{\lambda}(1 - \e^{-\lambda x} - \lambda z \e^{-\lambda z})\right) \\
  &= \frac{1}{\lambda} - \frac{z\e^{-\lambda z}}{1 - \e^{-\lambda z}} \\
  &= \frac{1}{\lambda} - \frac{z}{\exp\{\lambda z\} - 1}. \\
\end{split}
$$
Inserting this result into Expression \eqref{eq:c-e-x} yields
\begin{equation}\label{eq:c-e-x-final}
  \E\left[X_i\mid \vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)} \right] = u_iz_i + (1 - u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1}\right).
\end{equation}

Similarly for $Y_i,i=1,\ldots,n$, we have that
\begin{equation}\label{eq:c-e-y-final}
\begin{split}
  \E\left[Y_i\mid \vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)} \right]
  &=\E\left[Y_i\mid \max(X_i, Y_i) = z_i, I(X_i\geq Y_i) = u_i, \lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &=(1 - u_i)\E\left[Y_i\mid\max(X_i,Y_i) = z_i, X_i < Y_i,\lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &\quad + u_i\E\left[X_i\mid\max(X_i,Y_i) = z_i, X_i\geq Y_i,\lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &= (1 - u_i)\E\left[Y_i\mid Y_i = z_i\right] + u_i\E\left[Y_i\mid Y_i \leq z_i,\lambda_1 = \lambda_1^{(t)}\right] \\
  &=(1 - u_i)z_i + u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)}z_i\} - 1}\right).
\end{split}
\end{equation}

Thus, by substituting Expression \eqref{eq:c-e-x-final} and \eqref{eq:c-e-y-final} into the log-likelihood for the complete data $\vect x, \vect y$, conditional on the observed data $\vect z,\vect u$, we get
\begin{equation}\label{eq:c-log-lik}
\begin{split}
  Q(\lambda_0,\lambda_1\mid \lambda_0^{(t)}, \lambda_1^{(t)})
  &= n(\log \lambda_0 + \log\lambda_1) \\
  &\quad - \lambda_0\sum_{i=1}^n\left[u_iz_i + (1 - u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1}\right)\right] \\
  &\quad - \lambda_1\sum_{i=1}^n\left[(1 - u_i)z_i + u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)}z_i\} - 1}\right)\right].
\end{split}
\end{equation}

[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.

The M-step of the EM-algorithm is to maximize $Q(\lambda_0,\lambda_1\mid\lambda_0^{(t)}, \lambda_1^{(t)})$ with respect to $(\lambda_0,\lambda_1)$, and set the $(\lambda_0^{(t+1)}, \lambda_1^{(t+1)})$ equal to the maximizer of $Q$.
We find the maximizer by taking the partial derivatives of Expression \eqref{eq:c-log-lik}, setting them equal to zero.
We have
$$
\begin{split}
  \frac{\partial}{\partial \lambda_0} Q(\lambda_0,\lambda_1\mid\lambda_0^{(t)},\lambda_1^{(t)})
  &= \frac{n}{\lambda_0} - \sum_{i=1}^n\left[u_iz_i + (1 - u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1}\right)\right] = 0, \\
  \frac{\partial}{\partial \lambda_1} Q(\lambda_0,\lambda_1\mid\lambda_0^{(t)},\lambda_1^{(t)})
  &= \frac{n}{\lambda_1} - \sum_{i=1}^n\left[(1 - u_i)z_i + u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)} z_i\} - 1}\right)\right] = 0,
\end{split}
$$
which gives the maximizer
\begin{equation}\label{eq:m-step}
  \begin{pmatrix}\lambda_0^{(t+1)} \\[3mm] \lambda_1^{(t+1)}\end{pmatrix} =
  \begin{pmatrix}\left.n\middle/\sum_{i=1}^n\left[u_iz_i + (1 - u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1}\right)\right]\right. \\[5mm]
  \left.n\middle/\sum_{i=1}^n\left[(1 - u_i)z_i + u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)} z_i\} - 1}\right)\right]\right.
  \end{pmatrix}.
\end{equation}

We then implement the EM-algorithm with the convergence criterion that

$$
  \left\|\vect \lambda^{(t + 1)} - \vect \lambda^{(t)}\right\|_2 < \epsilon = 10^{-10},\quad \vect \lambda^{(t)} = (\lambda_0^{(t)}, \lambda_1^{(t)}).
$$

```{r em-alg}
# Computes the expectation of the log-likelihood of the complete data given the
# observational data
# lambda:      (lambda_0^{(t)}, lambda_1^{(t)})
# lambda_next: (lambda_0^{(t + 1)}, lambda_1^{(t + 1)})
# z:           observed data, max(x, y)
# u:           observed data, I(x >= y)
Q_func <- function(lambda, lambda_next, z, u) {
  length(z)*(log(lambda_next[1]) + log(lambda_next[2])) -
    lambda_next[1]*sum(u*z + (1 - u)*(1/lambda[1] - z/(exp(lambda[1]*z) - 1))) -
    lambda_next[2]*sum((1 - u)*z + u*(1/lambda[2] - z/(exp(lambda[2]*z) - 1)))
}

# Computes the MLE of lambda_0 and lambda_1 given the observed data using the EM-algorithm
# Additionally, it returns the list of recorded log-likelihood values from the iterations
# z:       observed data, max(x, y)
# u:       observed data, I(x >= y)
# lambda:  starting values (lambda_0^{(0)}, lambda_1^{(0)})
# itermax: maximum number of iterations
# tol:     convergence tolerance value 
EM <- function(z, u, lambda, itermax = 300, tol = 1e-10) {
  lambda_next <- numeric(2)
  log_liks <- numeric(0)
  for (i in 1:itermax) {
    # update (lambda_0^{(t + 1)}, lambda_1^{(t + 1)})
    lambda_next[1] <- 1 / mean(u*z + (1 - u)*(1/lambda[1] - z/(exp(lambda[1]*z) - 1)))
    lambda_next[2] <- 1 / mean((1 - u)*z + u*(1/lambda[2] - z/(exp(lambda[2]*z) - 1)))
    log_liks <- c(log_liks, Q_func(lambda, lambda_next, z, u))  # add Q-value
    # check convergence
    if(norm(lambda_next - lambda, type = "2") < tol) {
      break
    }
    lambda <- lambda_next  # update (lambda_0^{(t)}, lambda_1^{(t)})
  }
  list(lambda = lambda_next, log_liks = log_liks)
}
```
Since we only see the maximum of $x$ and $y$, we expect that $\E[X] < \E[Z\mid U=1]$ and $\E[Y]< \E[Z\mid U=0]$.

```{r mean-observed}
cat(sprintf("mean of z given u = 1: %.1f\nmean of z given u = 0: %.1f",
            mean(z[u == 1]), mean(z[u == 0])))
```
We then use the starting values $(\lambda_0^{(0)}, \lambda_1^{(0)}) = (1/0.4, 1/0.2) = (2.5, 5)$, expecting the final estimates $(\hat\lambda_0,\hat\lambda_1)$ to be larger.

```{r}
EM_result <- EM(z, u, c(2.5, 5), tol = 1e-10)
cat(sprintf("Estimate of lambda_%d: %.5f\n", 0:1, EM_result$lambda))
```

These estimates $(\hat\lambda_0,\hat\lambda_1)$ are slightly higher than the initial values $(\lambda_0^{(0)}, \lambda_1^{(0)})$ as expected.

A convergence plot of the $Q(\cdot)$-function is seen in Figure \ref{fig:em-convergence}.
We see that it converges very fast.
After 6 iterations, it stabilizes, while making small adjustments as we had set a quite strict convergence criterion.

```{r em-convergence, fig.align='center', fig.width=5, fig.height=3, fig.cap="Convergence plot of estimated expectation of the joint log likelihood for the complete data conditional on the observed data."}
tibble(index = 1:length(EM_result$log_liks), log_liks = EM_result$log_liks) %>%
  ggplot(aes(index, log_liks)) +
  geom_point() +
  labs(x = "iteration", y = expression(Q(lambda^{t+1}*"|"*lambda^t))) +
  theme_minimal()
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.

To get an idea of the biasedness and confidence in the estimated values, we perform a parametric bootstrap.
The procedure is shown in Algorithm \ref{alg:c-bootstrap}.

\begin{algorithm}[H]\label{alg:c-bootstrap}
\SetAlgoLined
\KwResult{Standard deviations, biases and correlation of $\hat\lambda_0,\hat\lambda_1$}
  Set $(\lambda_0^{(0)},\lambda_1^{(0)})\leftarrow(\hat\lambda_0,\hat\lambda_1)$\;
  Set $n\leftarrow200$\;
  Initialize $\vect\lambda$-List of length $B$\;
  \For{$i\leftarrow 1$ \KwTo $B$}{
    Draw $x_1^*, \ldots, x_n^*\sim\Exp(\hat\lambda_0)$\;
    Draw $y_1^*, \ldots, y_n^*\sim\Exp(\hat\lambda_1)$\;
    Initialize $n$-vectors $\vect z^*$ and $\vect u^*$\;
    \For{$j\leftarrow 1$ \KwTo $n$}{
      $z_j^*\leftarrow \max(x_j^*, y_j^*)$\;
      $u_j^*\leftarrow I(x_j^*\geq y_j^*)$\;
    }
    $(\hat\lambda_0^*, \hat\lambda_1^*)\leftarrow$ EM-algorithm$\left(\vect z^*,\vect u^*, (\lambda_0^{(0)}, \lambda_1^{(0)})\right)$\;
    $\vect \lambda$-list[$i$]$\leftarrow(\hat\lambda_0^*, \hat\lambda_1^*)$\;
  }
  Compute sample standard deviation $(\hat\sigma_{\lambda_0},\hat\sigma_{\lambda_1})$ from $\vect\lambda$-list\;
  Compute sample mean $(\hat\mu_{\lambda_0},\hat\mu_{\lambda_1})$ from $\vect\lambda$-list\;
  bias$_{(\hat\lambda_0,\hat\lambda_1)}\leftarrow (\hat\mu_{\lambda_0},\hat\mu_{\lambda_1}) - (\hat\lambda_0,\hat\lambda_1)$\;
  Compute sample correlation $\hat\rho$ from $\vect\lambda$-list\;
 \caption{Parametric bootstrapping for inference of $(\hat\lambda_0,\hat\lambda_1)$}
\end{algorithm}

We implement this algorithm below.

```{r param-bootstrap}
# performs a parametric bootstrap of the estimated parameters, and returns an estimate of
# the standard deviation, bias, and correlation of (hat_lambda_0, hat_lambda_1)
# lamba_hat: estimated parameters from the EM-algorithm
# B:         number of bootstrap samples
# itermax:   maximum iterations in the EM-algorithm
# tol:       convergence tolerance in the EM-algorithm
param_boot <- function(lambda_hat, B = 10000, itermax = 300, tol = 1e-5) {
  n <- 200  # number of observations
  lambda_list <- matrix(numeric(2*B), nrow = B)
  for (i in 1:B) {
    x_star <- rexp(n, lambda_hat[1])  # draw x*
    y_star <- rexp(n, lambda_hat[2])  # draw y*
    z_star <- pmax(x_star, y_star)    # compute z = max(x, y)
    u_star <- 1*(x_star >= y_star)    # compute u = I(x >= y)
    lambda_star <- EM(z_star, u_star, lambda_hat, itermax, tol)$lambda  # get estimate
    lambda_list[i,] <- lambda_star                                      # add estimate
  }
  sigma_hat <- apply(lambda_list, 2, sd)              # compute sample standard deviation
  mu_hat <- apply(lambda_list, 2, mean)               # compute sample mean
  bias <- mu_hat - lambda_hat                         # compute the bias
  rho_hat <- cor(lambda_list[, 1], lambda_list[, 2])  # compute the sample correlation
  list(sample_sd = sigma_hat, bias = bias, sample_cor = rho_hat)
}
```

We get the following estimates of $\SD[\hat\lambda_0], \SD[\hat\lambda_1], \Bias[\hat\lambda_0], \Bias[\hat\lambda_1]$ and $\Corr[\hat\lambda_0,\hat\lambda_1]$.

```{r compute-param-boot}
set.seed(93)
result <- param_boot(EM_result$lambda)
result
```

The standard deviations are of order $10^{-1}$, giving us a fair confidence in the estimated parameters.
The estimated correlation is very weak, practically speaking it is zero, which is expected as there is no reason for the parameters to be related in any way, as all the computations of the two values are done separately.
However, we notice that there is a small positive bias, which means that the ML estimators are larger than their expected values.
As we are interested in the true values of the parameters $\lambda_0$ and $\lambda_1$, we would prefer to go for a bias-corrected estimate.

[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 4.

We wish to find an analytic formula for $f_{Z_i, \, U_i}(z_i, u_i \mid \lambda_0, \lambda_1)$. Omitting the indices for $Z_i, U_i, X_i$ and $Y_i$ for computational convenience, we get
$$
\begin{split}
 f_{Z,U}(z,u\mid\lambda_0,\lambda_1) &=
 \Pr(\max(X,Y)=z, I(X\geq Y) = u\mid\lambda_0,\lambda_1) \\
 &=u\Pr(\max(X, Y) = z, X\geq Y\mid\lambda_0,\lambda_1) + (1 - u)\Pr(\max(X, Y) = z,Y>X\mid\lambda_0,\lambda_1) \\
 &=u\Pr(X = z, X\geq Y\mid\lambda_0,\lambda_1) + (1 - u)\Pr(Y = z,Y > X\mid\lambda_0,\lambda_1) \\
 &=uf_X(z\mid\lambda_0,\lambda_1)\Pr(X\geq Y\mid X=z,\lambda_0,\lambda_1) \\
 &\quad + (1 - u)f_Y(z\mid\lambda_0,\lambda_1)\Pr(Y > X\mid Y=z,\lambda_0,\lambda_1) \\
 &= uf_X(z\mid\lambda_0,\lambda_1)F_Y(z\mid\lambda_0,\lambda_1) + (1 - u)f_Y(z\mid\lambda_0,\lambda_1)F_X(x\mid\lambda_0,\lambda_1) \\
 &= u\lambda_0\e^{-\lambda_0z}(1 - \e^{-\lambda_1z}) + (1 - u)\lambda_1\e^{-\lambda_1z}(1 - \e^{-\lambda_0z}).
\end{split}
$$

We also want to find the maximum likelihood estimators $\hat{\lambda}_0$ and $\hat{\lambda}_1$. The likelihood function is given by
$$
  L(\lambda_0, \lambda_1 \mid \bfz, \bfu) = \prod_{i=1}^n f_{Z_i, \, U_i}(z_i, u_i \mid \lambda_0, \lambda_1) = \prod_{i \colon u_i=0} \lambda_1 \e^{-\lambda_1 z_i} ( 1 - \e^{-\lambda_0 z_i} ) \prod_{i \colon u_i=1} \lambda_0 \e^{-\lambda_0 z_i} ( 1 - \e^{-\lambda_1 z_i} ).
$$
The log-likelihood function is then
$$
  \ell(\lambda_0, \lambda_1 \mid \bfz, \bfu) = \sum_{i \colon u_i=0} \ln[\lambda_1 \e^{-\lambda_1 z_i} ( 1 - \e^{-\lambda_0 z_i} )] + \sum_{i \colon u_i=1} \ln[\lambda_0 \e^{-\lambda_0 z_i} ( 1 - \e^{-\lambda_1 z_i} )].
$$
We optimize this in the following code block using the `optim()` function in `R`.

```{r MLE log-likelihood}

# Defining the log-likelihood function. Note that it has to be defined
# as the negative of what was discussed earlier to make optim() find
# the maximum value, and not the minimum:
log_likelihood <- function(lambdas, z, u) {
  u0s <- which(u == 0)  # Indices where u = 0
  u1s <- which(u == 1)  # Indices where u = 1
  ll <- sum(log(lambdas[2] * exp(-lambdas[2]*z[u0s]) * (1 - exp(-lambdas[1]*z[u0s])))) +
        sum(log(lambdas[1] * exp(-lambdas[1]*z[u1s]) * (1 - exp(-lambdas[2]*z[u1s]))))
  return(-ll)
}

# Doing the optimization:
MLE <- optim(par = c(1, 1), fn = log_likelihood, z = z, u = u)
MLE$par

```

We see then that the maximum likelihood estimators are $\hat{\boldsymbol{\lambda}}_\mathrm{MLE} = \begin{bmatrix} 3.465890 & 9.351103 \end{bmatrix}^\top$. The absolute difference between the MLE and the EM estimators are shown below.

```{r MLE and EM}

abs(MLE$par - EM_result$lambda)

```

Se we see that the difference between the two estimators are small.

There are some advantages in optimizing the likelihood directly compared to using the EM algorithm. One of the advantages is that it is generally more efficient, as the EM algorithm can be very slow, especially depending on the "amount" of missing data. Also for MLE, only one optimization is necessary, while for the EM algorithm, the parameterized function may need to be optimized iteratively. In addition to this, when optimizing the likelihood one can derive the Hessian at the curvature, which then can give the standard errors without having to use a bootstrapping.



[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# References