---
title: "Exercise 3"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  github_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(
  echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE, strip.white = TRUE,
  prompt = FALSE, cache = TRUE, size = "scriptsize", fig.width = 6, fig.height = 4
)
knitr::opts_knit$set(root.dir = "./Additional files")  # Changed working directory
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Poisson}{\operatorname{Poisson}}
\newcommand{\Exp}{\operatorname{Exponential}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\Betadist}{\operatorname{Beta}}
\newcommand{\Gammadist}{\operatorname{Gamma}}
\newcommand{\InvGamma}{\operatorname{Inv-Gamma}}
\newcommand{\Normal}{\operatorname{Normal}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bfzero}{\mathbf{0}}
\newcommand{\bfeta}{\boldsymbol{\eta}}
\newcommand{\dtheta}{\, \mathrm{d} \theta}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}  % vector
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}  % matrix


```{r libraries and help files, eval = TRUE, echo = FALSE}
library(tidyverse)  # Collection of R packages designed for data science

# Extract pre-programmed R-code
source("probAdata.R")  # Code
source("probAhelp.R")  # Data
```

# Problem A: Comparing AR(2) parameter estimators using resampling of residuals

## Subproblem 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem B: Permutation test

## Subproblem 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.

[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.

[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem C: The EM-algorithm and bootstrapping

Let $x_1,\ldots, x_n$ and $y_1,\ldots, y_n$ be independent random variables, with $x_i\sim\Exp(\lambda_0)$ and $y_i\sim\Exp(\lambda_1)$, for $i=1,\ldots n$.
Assume that we do not observe $(x_i, y_i), i=1,\ldots,n$, directly.
Instead, we observe
\begin{equation}\label{eq:c-observed}
  \begin{split}
    z_i &= \max(x_i, y_i),\quad \text{for}\quad i=1,\ldots, n, \\
    u_i &= I(x_i \geq y_i),\quad \text{for}\quad i=1,\ldots, n,
  \end{split}
\end{equation}

where $I(\cdot)\in\{0, 1\}$ is the indicator function. A histogram of the observed data is shown in Figure \ref{fig:em-observed}.

Based on the observed values $(z_i, u_i), i=1,\ldots,n$, we will use the EM-algorithm to find the MLE for $(\lambda_0, \lambda_1)$.

```{r em-observed, echo = FALSE, fig.align='center', fig.width=5, fig.height=3, fig.cap="The observed data"}
## Read observed data
z <- scan("z.txt")  # max(x, y)
u <- scan("u.txt")  # I(x >= y)

tibble(z = z, u = factor(u)) %>% ggplot(aes(x = z, fill = u)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 50) +
  labs(fill = expression(u==I(x >= y)), x = "z = max(x, y)") +
  theme_minimal()
```


## Subproblem 1.
Let $\vect x = (x_1,\ldots,x_n), \vect y = (y_1,\ldots, y_n), \vect z = (z_1, \ldots, z_n)$ and $\vect u = (u_1,\ldots, u_n)$.
For the E-step we compute the expectation of the joint log likelihood for the complete data conditioned on the observed data.
$$
\begin{split}
 Q(\lambda_0, \lambda_1\mid \lambda_0^{(t)},\lambda_1^{(t)}) &= \E\left[\log \mathcal L(\lambda_0,\lambda_1\mid \vect X, \vect Y) \mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\
 &= \E\left[\log f(\vect X, \vect Y\mid \lambda_0, \lambda_1)\mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\
 &= \E\left[\log\left(\prod_{i=1}^n f_X(X_i\mid\lambda_0)\cdot f_Y(Y_i\mid\lambda_1)\right)\mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\
 &= \E\left[\sum_{i=1}^n\log\left(\lambda_0\e^{-\lambda_0X_i}\cdot\lambda_1\e^{-\lambda_1Y_i}\right)\mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\
 &= \E\left[\sum_{i=1}^n\left(\log \lambda_0 + \log \lambda_1 - \lambda_0X_i - \lambda_1Y_i\right)\mid \vect z, \vect u, \lambda_0^{(t)}, \lambda_1^{(t)}\right] \\\\
 &= n(\log\lambda_0 + \log \lambda_1) \\
 &\quad -\lambda_0\sum_{i=1}^n\E\left[X_i\mid\vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)}\right] \\
 &\quad - \lambda_1\sum_{i=1}^n\E\left[Y_i\mid\vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)}\right].
\end{split}
$$
To evaluate the expectations in the last equality, we first do some simplifications.
For $X_i,i=1,\ldots, n$, we have the following:
\begin{equation}\label{eq:c-e-x}
\begin{split}
  \E\left[X_i\mid \vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)} \right]
  &=\E\left[X_i\mid \max(X_i, Y_i) = z_i, I(X_i\geq Y_i) = u_i, \lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &=u_i\E\left[X_i\mid\max(X_i,Y_i) = z_i, X_i\geq Y_i,\lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &\quad + (1 - u_i)\E\left[X_i\mid\max(X_i,Y_i) = z_i, X_i< Y_i,\lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &= u_i\E\left[X_i\mid X_i = z_i\right] + (1 - u_i)\E\left[X_i\mid X_i < z_i,\lambda_0 = \lambda_0^{(t)}\right] \\
  &=u_iz_i + (1 - u_i)\E\left[X_i\mid X_i < z_i,\lambda_0 = \lambda_0^{(t)}\right].
\end{split}
\end{equation}
The expectation in the last equality is found by first computing the conditional cdf of $[X\mid X < z, \lambda]$, omitting the subscripts for computational convenience, as we will use the result for $Y_i,i=1,\ldots, n$ as well.

$$
\begin{split}
  F(x\mid X < z,\lambda) &= \Pr(X < x\mid X < z,\lambda) \\
  &= \frac{\Pr(X < x, X < z\mid\lambda)}{\Pr(X< z\mid \lambda)} \\
  &= \frac{F_X(\min(x, z)\mid\lambda)}{F_X(z\mid \lambda)},
\end{split}
$$
where $F_X(\cdot\mid\lambda)\sim\Exp(\lambda)$.
The conditional pdf is then given by
$$
f(x\mid X< z,\lambda) = \frac{d}{dx}\frac{F_X(\min(x, z)\mid\lambda)}{F_X(z\mid \lambda)} = \frac{f_X(x\mid \lambda)}{F_X(z\mid \lambda)},\quad 0<x<z,
$$
giving the conditional expectation
$$
\begin{split}
  \E\left[X\mid X < z,\lambda\right] &= \int_0^z x f(x\mid X< z,\lambda)\,\mathrm{d}x \\
  &= \int_0^z x \frac{f_X(x\mid \lambda)}{F_X(z\mid \lambda)}\,\mathrm{d}x \\
  &= \int_0^z x\frac{\lambda\e^{-\lambda x}}{1 - \e^{-\lambda z}}\,\mathrm d x \\
  &= \frac{1}{1-\e^{-\lambda x}}\int_0^z \lambda x\e^{-\lambda x}\,\mathrm d x \\
  &= \frac{1}{1-\e^{-\lambda x}}\left(\frac{1}{\lambda}(1 - \e^{-\lambda x} - \lambda z \e^{-\lambda z})\right) \\
  &= \frac{1}{\lambda} - \frac{z\e^{-\lambda z}}{1 - \e^{-\lambda z}} \\
  &= \frac{1}{\lambda} - \frac{z}{\exp\{\lambda z\} - 1}. \\
\end{split}
$$
Inserting this result into Expression \eqref{eq:c-e-x} yields
\begin{equation}\label{eq:c-e-x-final}
  \E\left[X_i\mid \vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)} \right] = u_iz_i + (1 - u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1}\right).
\end{equation}

Similarly for $Y_i,i=1,\ldots,n$, we have that
\begin{equation}\label{eq:c-e-y-final}
\begin{split}
  \E\left[Y_i\mid \vect z,\vect u, \lambda_0^{(t)},\lambda_1^{(t)} \right]
  &=\E\left[Y_i\mid \max(X_i, Y_i) = z_i, I(X_i\geq Y_i) = u_i, \lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &=(1 - u_i)\E\left[Y_i\mid\max(X_i,Y_i) = z_i, X_i < Y_i,\lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &\quad + u_i\E\left[X_i\mid\max(X_i,Y_i) = z_i, X_i\geq Y_i,\lambda_0 = \lambda_0^{(t)},\lambda_1 = \lambda_1^{(t)}\right] \\
  &= (1 - u_i)\E\left[Y_i\mid Y_i = z_i\right] + u_i\E\left[Y_i\mid Y_i \leq z_i,\lambda_1 = \lambda_1^{(t)}\right] \\
  &=(1 - u_i)z_i + u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)}z_i\} - 1}\right).
\end{split}
\end{equation}

Thus, by substituting Expression \eqref{eq:c-e-x-final} and \eqref{eq:c-e-y-final} into the log-likelihood for the complete data $\vect x, \vect y$, conditional on the observed data $\vect z,\vect u$, we get
\begin{equation}\label{eq:c-log-lik}
\begin{split}
  Q(\lambda_0,\lambda_1\mid \lambda_0^{(t)}, \lambda_1^{(t)})
  &= n(\log \lambda_0 + \log\lambda_1) \\
  &\quad - \lambda_0\sum_{i=1}^n\left[u_iz_i + (1 - u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1}\right)\right] \\
  &\quad - \lambda_1\sum_{i=1}^n\left[(1 - u_i)z_i + u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)}z_i\} - 1}\right)\right].
\end{split}
\end{equation}

[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.

The M-step of the EM-algorithm is to maximize $Q(\lambda_0,\lambda_1\mid\lambda_0^{(t)}, \lambda_1^{(t)})$ with respect to $(\lambda_0,\lambda_1)$, and set the $(\lambda_0^{(t+1)}, \lambda_1^{(t+1)})$ equal to the maximizer of $Q$.
We find the maximizer by taking the partial derivatives of Expression \eqref{eq:c-log-lik}, setting them equal to zero.
We have
$$
\begin{split}
  \frac{\partial}{\partial \lambda_0} Q(\lambda_0,\lambda_1\mid\lambda_0^{(t)},\lambda_1^{(t)})
  &= \frac{n}{\lambda_0} - \sum_{i=1}^n\left[u_iz_i + (1 - u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1}\right)\right] = 0, \\
  \frac{\partial}{\partial \lambda_1} Q(\lambda_0,\lambda_1\mid\lambda_0^{(t)},\lambda_1^{(t)})
  &= \frac{n}{\lambda_1} - \sum_{i=1}^n\left[(1 - u_i)z_i + u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)} z_i\} - 1}\right)\right] = 0,
\end{split}
$$
which gives the maximizer
\begin{equation}\label{eq:m-step}
  \begin{pmatrix}\lambda_0^{(t+1)} \\[3mm] \lambda_1^{(t+1)}\end{pmatrix} =
  \begin{pmatrix}\left.n\middle/\sum_{i=1}^n\left[u_iz_i + (1 - u_i)\left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1}\right)\right]\right. \\[5mm]
  \left.n\middle/\sum_{i=1}^n\left[(1 - u_i)z_i + u_i\left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)} z_i\} - 1}\right)\right]\right.
  \end{pmatrix}.
\end{equation}

We then implement the EM-algorithm with the convergence criterion that

$$
  \left\|\vect \lambda^{(t + 1)} - \vect \lambda^{(t)}\right\|_2 < \epsilon = 10^{-10},\quad \vect \lambda^{(t)} = (\lambda_0^{(t)}, \lambda_1^{(t)}).
$$

```{r em-alg}

Q_func <- function(lambda, lambda_next, z, u) {
  length(z)*(log(lambda_next[1]) + log(lambda_next[2])) -
    lambda_next[1]*sum(u*z + (1 - u)*(1/lambda[1] - z/(exp(lambda[1]*z) - 1))) -
    lambda_next[2]*sum((1 - u)*z + u*(1/lambda[2] - z/(exp(lambda[2]*z) - 1)))
}

EM <- function(z, u, lambda, itermax = 100, tol = 1e-5) {
  lambda_next <- numeric(2)
  log_liks <- numeric(0)
  for (i in 1:itermax) {
    lambda_next[1] <- 1 / mean(u*z + (1 - u)*(1/lambda[1] - z/(exp(lambda[1]*z) - 1)))
    lambda_next[2] <- 1 / mean((1 - u)*z + u*(1/lambda[2] - z/(exp(lambda[2]*z) - 1)))
    log_liks <- c(log_liks, Q_func(lambda, lambda_next, z, u))
    if(norm(lambda_next - lambda, type = "2") < tol) {
      break
    }
    lambda <- lambda_next
  }
  list(lambda = lambda_next, log_liks = log_liks)
}
```
Looking at Figure \ref{fig:em-observed}, we can make an initial guess for the rate parameters.
Since we only see the maximum of $x$ and $y$, we expect that $\E[X] < \E[Z\mid U=1]$ and $\E[Y]< \E[Z\mid U=0]$.

```{r mean-observed}
cat(sprintf("mean of z given u = 1: %.1f\nmean of z given u = 0: %.1f",
            mean(z[u == 1]), mean(z[u == 0])))
```
We then use the starting values $(\lambda_0^{(0)}, \lambda_1^{(0)}) = (1/0.4, 1/0.2) = (2.5, 5)$, expecting the final estimates $(\hat\lambda_0,\hat\lambda_1)$ to be larger.

```{r}
EM_result <- EM(z, u, c(2.5, 5), tol = 1e-10)
cat(sprintf("Estimate of lambda_%d: %.2f\n", 0:1, EM_result$lambda))
```

We get the estimates
$$
 (\hat\lambda_0, \hat\lambda_1) = (3.47, 9.35),
$$
which is higher than the initial values as expected.

A convergence plot of the $Q(\cdot)$-function is seen in Figure \ref{fig:em-convergence}.
We see that it converges very fast.
After 6 iterations, it stabilizes, while making small adjustments as we had set a quite strict convergence criterion.

```{r em-convergence, fig.align='center', fig.width=5, fig.height=3, fig.cap="Convergence plot of estimated expectation of the joint log likelihood for the complete data conditional on the observed data."}
tibble(index = 1:length(EM_result$log_liks), log_liks = EM_result$log_liks) %>%
  ggplot(aes(index, log_liks)) +
  geom_point() +
  labs(x = "iteration", y = expression(Q(lambda^{t+1}*"|"*lambda^t))) +
  theme_minimal()
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.

[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 4.

[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------