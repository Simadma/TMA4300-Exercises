---
title: "Exercise 2"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  github_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
bibliography: ref.bib
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(
  echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE, strip.white = TRUE,
  prompt = FALSE, cache = TRUE, size = "scriptsize", fig.width = 6, fig.height = 4,
  fig.align = 'center'
)
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Poisson}{\operatorname{Poisson}}
\newcommand{\Exp}{\operatorname{Exponential}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\Betadist}{\operatorname{Beta}}
\newcommand{\Gammadist}{\operatorname{Gamma}}
\newcommand{\InvGamma}{\operatorname{Inv-Gamma}}
\newcommand{\Normal}{\operatorname{Normal}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bfzero}{\mathbf{0}}
\newcommand{\bfeta}{\boldsymbol{\eta}}
\newcommand{\dtheta}{\, \mathrm{d} \theta}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\e}{\mathrm{e}}


```{r libraries, eval = TRUE, echo = FALSE}
library(tidyverse)  # Collection of R packages designed for data science

library(GGally)     # Extension to 'ggplot2'

library(INLA)       # Full Bayesian Analysis of Latent Gaussian Models using Integrated
                    # Nested Laplace Approximations
library(mvtnorm)    # For multivariate normal distribution
library(matrixStats)  # For column-wise variance of a matrix
```

# Problem A: The coal-mining disaster data

In this problem, we analyze a data set from [@jarrett1979note] on the dates of 191 explosions in coal mines in UK involving 10 or more fatalities, covering the period 15 March, 1851 to 22 March, 1962 inclusive.

## Subproblem 1.
Figure \ref{fig:coal} shows the 191 explosions over the period. It appears to be a steep linear trend from the beginning to about year 1890  where there is a sudden decrease in the rate. This suggests that there has been a major improvement in the safety regulations. By digging a little deeper, we find that according to [@mills2010regulating], in the late 1890s, all underground extractive industries in the UK, were subjected to varying degrees of regulation and control, and the inspectorate had greatly expanded in numbers, experience and authority.

```{r coal, fig.width=4, fig.height=3, fig.cap="\\label{fig:coal}Cumulative plot of coal mines explosions from the period 15 March, 1851 to 22 March, 1962."}
data(coal, package = "boot")
ggplot(cbind(coal, explosions = 1:nrow(coal)), aes(date, explosions)) +
  geom_step(aes(color = "Cumulative explosions")) +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank())
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.
We assume the coal-mining disasters follow an inhomogeneous Poisson process with intensity function $\lambda(t)$ (number of events per year).
We assume $\lambda(t)$ to be piecewise constant with $n$ breakpoints at times
$t_k:k=1,\ldots,n$.
We let $t_0$ and $t_{n+1}$ be the start and end times corresponding to 15 March, 1851 and 22 March, 1962, respectively, and let $t_0<t_1<t_2<\ldots<t_n<t_{n+1}$.
Then the intensity function becomes
\begin{equation}\label{eq:intensity}
  \lambda(t) = \begin{cases}
    \lambda_{k-1}, & \text{if}\,t\in[t_{k-1},t_k),\quad k=1,\ldots,n, \\
    \lambda_n, & \text{if}\,t\in[t_n,t_{n+1}].
  \end{cases}
\end{equation}

Furthermore, let $x_k$ be the $y_k$-vector of the exact times when the disasters occurred in the time interval $[t_k, t_{k+1})$.
We then let $x$ be the $y$-vector of the exact times when the disasters occurred in the whole time interval $[t_0, t_{n+1}]$, where $y = \sum_{k=0}^n y_k$.
We assume that the coal-mining disasters follow a hierarchical Bayesian structure, with
\begin{equation}\label{eq:bayes-poisson}
  \begin{split}
    [y_k\mid\lambda_k, t_k, t_{k+1}]&\sim\Poisson(\lambda_k(t_{k+1} - t_k)),\quad k=0,1,\ldots, n,\\
    [x_k\mid y_k,t_{k},t_{k+1}]&\sim \frac{y_k!}{(t_{k+1} - t_k)^{y_k}},
    \quad\quad\quad\quad\quad k=0,1,\ldots, n,\\
    \Rightarrow\quad [x_k,y_k\mid\lambda_k,t_{k},t_{k+1}]&\sim \lambda_k^{y_k}\exp(-\lambda_k(t_{k+1} - t_k)),
    \; k=0,1,\ldots, n, \\
    t_k &\sim \Uniform(t_{k-1}, t_{k+1}),\quad\, \quad k=1,2,\ldots, n, \\
    [\lambda_k\mid\beta]&\sim \Gammadist(2, 1/\beta),\quad\quad\quad\quad k=0,1,\ldots,n, \\
    \beta &\sim f_\beta(\beta)\propto \frac{\exp\left(-\frac{1}{\beta}\right)}{\beta},\quad\beta>0,
  \end{split}
\end{equation}
where $\lambda_0,\ldots,\lambda_n$ are conditionally independent of each other with respect to $\beta$ and independent of $t_1,\ldots,t_n$, where the first and second argument in $\Gammadist(\cdot, \cdot)$ corresponds to the shape and rate parameter, respectively, and where $f_\beta(\beta)$ is an improper prior.

We will in this problem assume $n=1$, such that the model parameters are $\theta = (t_1, \lambda_0, \lambda_1, \beta)$.
The posterior distribution for $\theta$ given the observed data $x$ is then given by
$$
\begin{split}
  f_{\theta\mid x}(\theta\mid x)
  &\propto f_{\theta, x,y}(\theta, x,y) \\
  &= f_{x,y\mid t_1,\lambda_0,\lambda_1}(x,y\mid t_1,\lambda_0,\lambda_1)\cdot f_{t_1}(t_1)\cdot f_{\lambda_0\mid \beta}(\lambda_0\mid\beta) \cdot f_{\lambda_1\mid \beta}(\lambda_1\mid\beta) \cdot f_\beta(\beta) \\
  &\propto \lambda_0^{y_0}\exp\{-\lambda_0(t_1 - t_0)\}\lambda_1^{y_1}\exp\{-\lambda_1(t_2 - t_1)\}\cdot\frac{1}{t_2 - t_0}\cdot \frac{\lambda_0}{\beta^2}\e^{-\frac{\lambda_0}{\beta}}\cdot\frac{\lambda_1}{\beta^2}\e^{-\frac{\lambda_1}{\beta}}\cdot \frac{\exp\left(-\frac{1}{\beta}\right)}{\beta} \\
  &\propto \frac{\lambda_0^{y_0 + 1}\lambda_1^{y_1 + 1}}{\beta^5} \exp\left\{-\left(\lambda_0(t_1 - t_0) + \lambda_1(t_2 - t_1) + \frac{1}{\beta}(\lambda_0 + \lambda_1 + 1)\right)\right\},\\
  &\text{with}\quad \beta > 0,\; \lambda_0\geq0,\;\lambda_1\geq0\quad\text{and}\quad t_0\leq t_1 \leq t_2.
\end{split}
$$
That is, the posterior distribution is equal to
\begin{equation}\label{eq:A-posterior}
  f_{\theta\mid x}(\theta\mid x) = \frac{1}{c}\cdot \frac{\lambda_0^{y_0 + 1}\lambda_1^{y_1 + 1}}{\beta^5} \exp\left\{-\left(\lambda_0(t_1 - t_0) + \lambda_1(t_2 - t_1) + \frac{1}{\beta}(\lambda_0 + \lambda_1 + 1)\right)\right\},
\end{equation}
where $c$ is the normalizing constant.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.
We calculate the full conditional for each of the elements in $\theta$.
$$
\begin{split}
  [t_1\mid\lambda_0,\lambda_1,\beta,x]
    &\sim f_{t_1\mid\lambda_0,\lambda_1,\beta,x}(t_1\mid\lambda_0,\lambda_1,\beta,x) \\
    &\propto f_{\theta\mid x}(\theta\mid x) \\
    &\propto I_{(t_0, t_2)}(t_1)\cdot\lambda_0^{y_0 + 1}\lambda_1^{y_1 + 1}\exp\left\{-(\lambda_0 - \lambda_1)t_1\right\} \\
    &\sim\text{Unknown distribution}; \\
  [\lambda_0\mid t_1,\lambda_1,\beta,x]
    &\sim f_{\lambda_0\mid t_1,\lambda_1,\beta,x}(\lambda_0\mid t_1,\lambda_1,\beta,x) \\
    &\propto f_{\theta\mid x}(\theta\mid x) \\
    &\propto I_{[0,\infty)}(\lambda_0)\cdot\lambda_0^{(y_0 + 2) - 1}\exp\left\{-\left(t_1 - t_0 + \frac{1}{\beta}\right)\lambda_0\right\} \\
    &\sim\Gammadist\left(y_0 + 2, t_1 - t_0 + \frac{1}{\beta}\right); \\
  [\lambda_1\mid t_1,\lambda_0,\beta,x]
    &\sim f_{\lambda_1\mid t_1,\lambda_0,\beta,x}(\lambda_1\mid t_1,\lambda_0,\beta,x) \\
    &\propto I_{[0,\infty)}(\lambda_1)\cdot f_{\theta\mid x}(\theta\mid x) \\
    &\propto \lambda_1^{(y_1 + 2) - 1}\exp\left\{-\left(t_2 - t_1 + \frac{1}{\beta}\right)\lambda_1\right\} \\
    &\sim \Gammadist\left(y_1 + 2, t_2 - t_1 + \frac{1}{\beta}\right); \\
  [\beta\mid t_1\lambda_0,\lambda_1,x]
    &\sim f_{\beta\mid t_1\lambda_0,\lambda_1,x}(\beta\mid t_1\lambda_0,\lambda_1,x) \\
    &\propto f_{\theta\mid x}(\theta\mid x) \\
    &\propto I_{(0,\infty)}(\beta)\cdot\beta^{-4-1}\exp\left\{-\frac{\lambda_0 + \lambda_1 + 1}{\beta}\right\} \\
    &\sim \InvGamma(4, \lambda_0 + \lambda_1 + 1).
\end{split}
$$


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 4.
We implement a single site McMC algorithm for $f(\theta\mid x)$, where we need to do a Gibbs step for all the full conditionals except for $[t_1\mid\lambda_0,\lambda_1,\beta,x]$, where we will do a Metropolis Hastings step. We choose a random walk proposal
\begin{equation}\label{eq:Asingle-prop}
[t_1^*\mid t_1^{(k)}]\sim\Normal(t_1^{(k)}, d),
\end{equation}
where $t_1^{(k)}$ is the current state of the Markov chain, and $d$ is a tuning parameter. We denote the proposal pdf by $Q(t_1^*\mid t_1^{(k)})$ and the target pdf by $\pi(t_1^{(k)}) = f_{t_1\mid \lambda_0,\lambda_1,\beta,x}(t_1^{(k)}\mid \lambda_0,\lambda_1,\beta,x)$. The acceptance probability is then given by
\begin{equation}\label{eq:Asingle-accept}
  \alpha(t_1^*\mid t_1^{(k)}) = \min\left\{1, \frac{\pi(t_1^*)\cdot Q(t_1^{(k)}\mid t_1^*)}{\pi(t_1^{(k)})\cdot Q(t_1^*\mid t_1^{(k)})}\right\}.
\end{equation}
Since the proposal distribution is a symmetric, we have that $Q(t_1^*\mid t_1^{(k)}) = Q(t_1^{(k)}\mid t_1^*)$. What is left to do, is to compute the ratio of the target distribution. First we notice that the change point $t_1$ directly affects the event counts $y_0$ and $y_1$. That is,
$$
  y_0(t) = \sum_{x}I(x < t)\quad\text{and}\quad y_1(t) = y - t_0(t).
$$
Let $y_i^{*} = y_i(t_1^*)$ and $y_i^{(k)} = y_i(t_1^{(k)})$. We compute the ratio

$$
\begin{split}
  \frac{\pi(t_1^*)}{\pi(t_1^{(k)})}
  &= \exp\left\{\log\frac{\lambda_0^{y_0^* + 1}\lambda_1^{y_1^* + 1}\e^{-(\lambda_0 - \lambda_1)t_1^*}}{\lambda_0^{y_0^{(k)} + 1}\lambda_1^{y_1^{(k)} + 1}\e^{-(\lambda_0 - \lambda_1)t_1^{k}}}\right\} \\
  &= \exp\left\{(y_0^* - y_0^{(k)})\log \lambda_0 + (y_1^* - y_1^{(k)})\log \lambda_1 -(\lambda_0 - \lambda_1)(t_1^* - t_1^{(k)})\right\},
\end{split}
$$
which we insert into \eqref{eq:Asingle-accept} to get the acceptance probability.

```{r single-site McMC algorithm}
# Performs a Metropolis Hastiings step to sample t_1
MH_t1 <- function(t_1, lambda_0, lambda_1, t_0, t_2, x, y_0, y_1, y, d) {
  accept <- 0
  t_star <- rnorm(1, mean = t_1, sd = d)  # Proposal
  if (t_0 < t_star & t_star < t_2) {
    y_0_star <- sum(x < t_star)  # Number of disasters before t_star
    y_1_star <- y - y_0_star     # Number of disasters after t_star
    alpha <- min(
      1,
      exp((y_0_star - y_0)*log(lambda_0) + (y_1_star - y_1)*log(lambda_1)
          - (lambda_0 - lambda_1)*(t_star - t_1))
    )
    if (runif(1) < alpha) {
      # Accept
      t_1 <- t_star
      y_0 <- y_0_star
      y_1 <- y_1_star
      accept <- 1
    }
  }
  list(t_1 = t_1, y_0 = y_0, y_1 = y_1, accept = accept)
}

# Samples from the posterior distribution using single site McMC
# data:     number of observations
# t_1:      initial changepoint
# lambda_0: initial rateparameter
# lambda_1: initial rateparameter
# beta:     initial hyperparameter
# d:        tuning parameter for the proposal distribution
# n_iter:   number of iterations
single_site_McMC <- function(data, t_1, lambda_0, lambda_1, beta, d = 1, n_iter = 5000) {
  x <- data$date
  y <- length(x)  # Number of disasters
  t_0 <- x[1]     # Start date
  t_2 <- x[y]     # End date
  theta <- matrix(nrow = n_iter, ncol = 4,
             dimnames = list(NULL, c("t_1", "lambda_0", "lambda_1", "beta"))
           )
  theta[1, ] <- c(t_1, lambda_0, lambda_1, beta)
  y_0 <- sum(x < t_1)  # Number of disasters before t_1
  y_1 <- y - y_0       # Number of disasters after t_1
  accept_count <- 0
  if (n_iter > 1) {
    for (i in 2:n_iter) {
      # Metropolis Hastings step
      param <- MH_t1(t_1, lambda_0, lambda_1, t_0, t_2, x, y_0, y_1, y, d)
      t_1 <- param$t_1
      y_0 <- param$y_0
      y_1 <- param$y_1
      accept_count <- accept_count + param$accept
      
      # Gibbs step, sample lambda_0
      lambda_0 <- rgamma(1,
                    shape = y_0 + 2,
                    rate  = t_1 - t_0 + 1/beta
                  )
      
      # Gibbs step, sample lambda_1
      lambda_1 <- rgamma(1,
                    shape = y_1 + 2,
                    rate  = t_2 - t_1 + 1/beta
                  )
      
      # Gibbs step, sample beta
      beta <- 1 / rgamma(1,
                    shape = 4,
                    rate  = lambda_0 + lambda_1 + 1
                  )
      # Update
      theta[i, ] <- c(t_1, lambda_0, lambda_1, beta)
    }
  }
  print(sprintf("Acceptance rate for t_1: %.2f%%", 100*accept_count/(n_iter - 1)))
  as.data.frame(theta)
}
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 5.
```{r run single-site, fig.cap="\\label{fig:single_site}Trace plot of two chains from different starting values using a single site McMC."}
set.seed(731)
# Trace plot function
# ...: Takes the chains as input
plot_trace <- function(...) {
  input_list <- list(...)
  n <- nrow(input_list[[1]])
  nchains <- length(input_list)
  bind_rows(..., .id = "chain") %>% 
    mutate(n = rep(1:n, nchains)) %>% 
    pivot_longer(c("t_1", "lambda_0", "lambda_1", "beta"), names_to = "parameter") %>% 
    ggplot(aes(n, value, color = chain)) +
    geom_line() +
    facet_wrap(~ parameter, nrow = 4, scales = "free_y") +
    theme_minimal()
}

# Sample two chains from different starting values
n <- 5000
theta1 <- single_site_McMC(coal, coal$date[1], 0.1, 10, 20, d = 10, n_iter = n)
theta2 <- single_site_McMC(coal, coal$date[nrow(coal)], 30, 0.1, 2, d = 10, n_iter = n)
plot_trace(theta1, theta2)
```

A trace plot of two chains from different starting values is shown in Figure \ref{fig:single_site}. The chain appears to converge instantly, with an acceptance rate just below $20\%$, which is a good rate as we want an acceptance rate somewhere between $20\%$ and $50\%$.
We let the burn-in period be the 300 first steps to be sure.

```{r single-site burnin,  fig.cap="\\label{fig:single_site_burnin}Trace plot of two chains from different starting values using a single site McMC after discarding the burn-in."}
burnin <- 300
theta1 <- tail(theta1, -burnin)
theta2 <- tail(theta2, -burnin)

# Plot trace
plot_trace(theta1, theta2)
```
Figure \ref{fig:single_site_burnin} shows the same trace plot but now after discarding the burn-in period.
The chain shows no trend.
We only see a homogeneous band from each of the parameters.
Looking specifically at the trace of $t_1$, it seems to be centered around 1890, which we anticipated by seeing a clear rate of change at the given point in Figure \ref{fig:coal}.
We also see that $t_1$ mixes quite well in the limiting distribution, but not as good as the other parameters.



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 6.
The proposal distribution given by \eqref{eq:Asingle-prop} has a tuning parameter $d$.
In Figure \ref{fig:single_site} and \ref{fig:single_site_burnin}, the tuning parameter was set to $d=10$.
We now try to set it at $d=1$ and $200$.
The result is shown in Figure \ref{fig:single-site-tune}.
For the parameters $\beta$ and $\lambda_1$, it is hard to see any difference.
Looking at the other parameters, $t_1$ and $\lambda_0$, we see that the chain with tuning parameter $d=200$ mixes almost instantly, but the chain with tuning parameter does not mix until $17500$ steps.
We also see that the acceptance rate is only $1\%$ for the chain with parameter $d=200$, while the chain with parameter $d=1$ has an acceptance rate of $50\%$.
After discarding the burn-in period, we can get a clearer picture of the behavior of the two chains in Figure \ref{fig:single-site-tune-burnin}.
The chain with $d=1$ takes very small frequent steps, and stays fixed around $t_1=1950$ for a long time before it moves to the neighborhood of $t_1=1890$.
Looking at Figure \ref{fig:coal}, one can see a small change of rate, so it makes sense that the distribution of $t_1$ has a small mode at this position.
The chain with $d=200$ takes very big steps, and therefore gets rejected almost all the time.

```{r single-site tuning, fig.cap="\\label{fig:single-site-tune}Trace plots of two chains with tuning parameters $d=1$ and $d=200$ using single-site McMC."}
set.seed(30)
n_iter = 20000
d <- c(1, 200)

# Sample two chains with different values for tuning parameter
theta3 <- single_site_McMC(coal, coal$date[nrow(coal)], 10, 10, 10,  d[1], n_iter)
theta4 <- single_site_McMC(coal, coal$date[nrow(coal)], 10, 10, 10,  d[2], n_iter)

# Plot trace
plot_trace(theta3, theta4) +
  scale_color_hue(name = "d", labels = d)
```

Figure \ref{fig:single-site-tune-burnin} shows the same trace plot, but after discarding the burn-in period.
We see that both of them has reached the limiting distribution, but they both yield a poor mixing, especially the chain with high tuning parameter.

```{r single-site tuning burn-in, fig.cap="\\label{fig:single-site-tune-burnin}Trace plots of two chains with tuning parameters $d=1$ and $d=200$ using single-site McMC after discarding the burn-in."}
burnin <- 17500
theta3 <- tail(theta3, -burnin)
theta4 <- tail(theta4, -burnin)

# Plot trace
plot_trace(theta3, theta4) +
  scale_color_hue(name = "d", labels = d)
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 7.

We will define and implement a block Metropolis-Hastings algorithm for $f_{\theta\mid x}(\theta\mid x)$ using two block proposals.

### Subsubproblem (a)

We will use a block proposal $(t_1, \lambda_0, \lambda_1)$ keeping $\beta$ unchanged.
We generate a proposal $(t_1^*, \lambda_0^*, \lambda_1^*)$ by proposing $t_1$ as before according to \eqref{eq:Asingle-prop}.
We then sample from the joint full conditional of $(\lambda_0^*,\lambda_1^*)$ including the new proposed value $t_1^*$. That is,
$$
\begin{split}
  t_1^*&\sim\Normal(t_1^{(k)}, d_t); \\
  [\lambda_0^*,\lambda_1^*\mid t_1^*,\beta, x]&\sim f_{\lambda_0,\lambda_1\mid t_1,\beta, x}(\lambda_0^*,\lambda_1^*\mid t_1^*,\beta, x) \\
  &\propto f_{\lambda_0\mid t_1,\beta, x}(\lambda_0^*\mid t_1^*,\beta, x)f_{\lambda_1\mid t_1,\beta, x}(\lambda_1^*\mid t_1^*,\beta, x) \\
  &= \Gammadist\left(\lambda_0;y_0^* + 2, t_1^* - t_0 + \frac{1}{\beta}\right)\Gammadist\left(\lambda_1;y_1^* + 2, t_2 - t_1^* + \frac{1}{\beta}\right),
\end{split}
$$
where $d_t$ is a tuning parameter.
The target distribution is given by

$$
\pi(t_1,\lambda_0,\lambda_1) \propto f_{\theta\mid x}(\theta\mid x)\propto\lambda_0^{y_0 + 1}\lambda_1^{y_1 + 1}\exp\left\{-\left(\lambda_0\left(t_1 - t_0 + \frac{1}{\beta}\right) + \lambda_1\left(t_2 - t_1 + \frac{1}{\beta}\right)\right)\right\},
$$
so the acceptance probability becomes

$$
\begin{split}
  &\quad\alpha(t_1^*,\lambda_0^*,\lambda_1^*\mid t_1^{(k)},\lambda_0^{(k)},\lambda_1^{(k)}) \\
  &=\min\left\{1, \frac{\pi(t_1^*,\lambda_0^*,\lambda_1^*)\cdot Q(t_1^{(k)},\lambda_0^{(k)},\lambda_1^{(k)}\mid t_1^*,\lambda_0^*,\lambda_1^*)}
  {\pi(t_1^{(k)},\lambda_0^{(k)},\lambda_1^{(k)})\cdot Q(t_1^*,\lambda_0^*,\lambda_1^*\mid t_1^{(k)},\lambda_0^{(k)},\lambda_1^{(k)})}\right\} \\
  &=\min\left\{1, \frac{\pi(t_1^*,\lambda_0^*,\lambda_1^*)\cdot\Gammadist\left(\lambda_0^{(k)};y_0^* + 2, t_1^* - t_0 + \frac{1}{\beta}\right)\Gammadist\left(\lambda_1^{(k)};y_1^* + 2, t_2 - t_1^* + \frac{1}{\beta}\right)}
  {\pi(t_1^{(k)},\lambda_0^{(k)},\lambda_1^{(k)})\cdot\Gammadist\left(\lambda_0^*;y_0^{(k)} + 2, t_1^{(k)} - t_0 + \frac{1}{\beta}\right)\Gammadist\left(\lambda_1^*;y_1^{(k)} + 2, t_2 - t_1^{(k)} + \frac{1}{\beta}\right)}\right\},
\end{split}
$$
where we have omitted the Normal proposal for $t_1^*$ because of the symmetry properties.


### Subsubproblem (b)
The other block proposal is $(\beta, \lambda_0, \lambda_1)$ keeping $t_1$ unchanged.
We generate a proposal $(\beta^*, \lambda_0^*, \lambda_1^*)$ by proposing $\beta^*$ from a Normal distribution centered at the current state $\beta$, like we did previously with $t_1$.
We then sample from the joint full conditional of $(\lambda_0^*,\lambda_1^*)$ including the new proposed value $\beta^*$.
That is,
$$
\begin{split}
  \beta^*&\sim\Normal(\beta^{(k)}, d_\beta); \\
  [\lambda_0^*,\lambda_1^*\mid t_1,\beta^*, x]&\sim f_{\lambda_0,\lambda_1\mid t_1,\beta, x}(\lambda_0^*,\lambda_1^*\mid t_1,\beta^*, x) \\
  &\propto f_{\lambda_0\mid t_1,\beta, x}(\lambda_0^*\mid t_1,\beta^*, x)f_{\lambda_1\mid t_1,\beta, x}(\lambda_1^*\mid t_1,\beta^*, x) \\
  &= \Gammadist\left(\lambda_0;y_0 + 2, t_1 - t_0 + \frac{1}{\beta^*}\right)\Gammadist\left(\lambda_1;y_1 + 2, t_2 - t_1 + \frac{1}{\beta^*}\right),
\end{split}
$$
where $d_\beta$ is a tuning parameter.
The target distribution is given by

$$
\pi(\beta,\lambda_0,\lambda_1) \propto f_{\theta\mid x}(\theta\mid x)\propto\frac{\lambda_0^{y_0 + 1}\lambda_1^{y_1 + 1}}{\beta^5}\exp\left\{-\left(\lambda_0(t_1 - t_0) + \lambda_1(t_2 - t_1) + \frac{1}{\beta}(\lambda_0 + \lambda_1 + 1)\right)\right\},
$$
so the acceptance probability becomes

$$
\begin{split}
  &\quad\alpha(\beta^*,\lambda_0^*,\lambda_1^*\mid \beta^{(k)},\lambda_0^{(k)},\lambda_1^{(k)}) \\
  &=\min\left\{1, \frac{\pi(\beta^*,\lambda_0^*,\lambda_1^*)\cdot Q(\beta^{(k)},\lambda_0^{(k)},\lambda_1^{(k)}\mid \beta^*,\lambda_0^*,\lambda_1^*)}
  {\pi(\beta^{(k)},\lambda_0^{(k)},\lambda_1^{(k)})\cdot Q(\beta^*,\lambda_0^*,\lambda_1^*\mid \beta^{(k)},\lambda_0^{(k)},\lambda_1^{(k)})}\right\} \\
  &=\min\left\{1, \frac{\pi(\beta^*,\lambda_0^*,\lambda_1^*)\cdot\Gammadist\left(\lambda_0^{(k)};y_0 + 2, t_1 - t_0 + \frac{1}{\beta^*}\right)\Gammadist\left(\lambda_1^{(k)};y_1 + 2, t_2 - t_1 + \frac{1}{\beta^*}\right)}
  {\pi(\beta^{(k)},\lambda_0^{(k)},\lambda_1^{(k)})\cdot\Gammadist\left(\lambda_0^*;y_0 + 2, t_1 - t_0 + \frac{1}{\beta^{(k)}}\right)\Gammadist\left(\lambda_1^*;y_1 + 2, t_2 - t_1 + \frac{1}{\beta^{(k)}}\right)}\right\},
\end{split}
$$
where we have omitted the Normal proposal for $\beta^*$ because of the symmetry properties.

```{r block algorithm}
# Computes the log target core of (t_1, lambda_0, lambda_1)
ltarget_1 <- function(t_1, lambda_0, lambda_1, beta, y_0, y_1, t_0, t_2) {
  (y_0 + 1)*log(lambda_0) + (y_1 + 1)*log(lambda_1) -
    (lambda_0*(t_1 - t_0 + 1/beta) + lambda_1*(t_2 - t_1 + 1/beta))
}

# Computes the log target core of (beta, lambda_0, lambda_1)
ltarget_2 <- function(t_1, lambda_0, lambda_1, beta, y_0, y_1, t_0, t_2) {
  (y_0 + 1)*log(lambda_0) + (y_1 + 1)*log(lambda_1) - 5*log(beta) -
    (lambda_0*(t_1 - t_0) + lambda_1*(t_2 - t_1) + 1/beta * (lambda_0 + lambda_1 + 1))
}

# Performs a Metropolis Hastiings step to sample (t_1, lambda_0, lambda_1)
MH_t1_lamb0_lamb1 <- function(t_1, lambda_0, lambda_1, beta, y_0, y_1, y, x, t_0, t_2, d)
{
  accept <- 0
  t_star <- rnorm(1, mean = t_1, sd = d)  # Proposal
  if (t_0 < t_star & t_star < t_2) {
    y_0_star <- sum(x < t_star)  # Number of disasters before t_star
    y_1_star <- y - y_0_star     # Number of disasters after t_star
    # Proposal
    lambda_0_star <- rgamma(1,
                            shape = y_0_star + 2,
                            rate  = t_star - t_0 + 1/beta
    )
    # Proposal
    lambda_1_star <- rgamma(1,
                            shape = y_1_star + 2,
                            rate  = t_2 - t_star + 1/beta
    )
    # Log target ratio
    ltarget_ratio <- ltarget_1(
      t_star, lambda_0_star, lambda_1_star, beta, y_0_star, y_1_star,
      t_0, t_2
    ) - ltarget_1(t_1, lambda_0, lambda_1, beta, y_0, y_1, t_0, t_2)
    # Log proposal ratio
    lprop_ratio <- dgamma(lambda_0,
                          shape = y_0_star + 2,
                          rate  = t_star - t_0 + 1/beta,
                          log   = TRUE
    ) + dgamma(lambda_1,
               shape = y_1_star + 2,
               rate  = t_2 - t_star + 1/beta,
               log   = TRUE
    ) - dgamma(lambda_0,
               shape = y_0 + 2,
               rate  = t_1 - t_0 + 1/beta,
               log   = TRUE
    ) - dgamma(lambda_1,
               shape = y_1 + 2,
               rate  = t_2 - t_1 + 1/beta,
               log   = TRUE
    )
    alpha <- min(1, exp(ltarget_ratio + lprop_ratio))
    if (runif(1) < alpha) {
      # Accept proposals
      t_1 <- t_star
      y_0 <- y_0_star
      y_1 <- y_1_star
      lambda_0 <- lambda_0_star
      lambda_1 <- lambda_1_star
      accept <- 1
    }
  }
  list(t_1 = t_1, y_0 = y_0, y_1 = y_1, lambda_0 = lambda_0, lambda_1 = lambda_1,
       accept = accept)
}

# Performs a Metropolis Hastiings step to sample (beta, lambda_0, lambda_1)
MH_beta_lamb0_lamb1 <- function(t_1, lambda_0, lambda_1, beta, y_0, y_1, t_0, t_2, d) {
  accept <- 0
  beta_star <- rnorm(1, mean = beta, sd = d)
  if (beta_star > 0) {
    # Proposal
    lambda_0_star <- rgamma(1,
                            shape = y_0 + 2,
                            rate  = t_1 - t_0 + 1/beta_star
    )
    # Proposal
    lambda_1_star <- rgamma(1,
                            shape = y_1 + 2,
                            rate  = t_2 - t_1 + 1/beta_star
    )
    # Log target ratio
    ltarget_ratio <- ltarget_2(
      t_1, lambda_0_star, lambda_1_star, beta_star, y_0, y_1, t_0, t_2
    ) - ltarget_2(t_1, lambda_0, lambda_1, beta, y_0, y_1, t_0, t_2)
    # Log proposal ratio
    lprop_ratio <- dgamma(lambda_0,
                          shape = y_0 + 2,
                          rate  = t_1 - t_0 + 1/beta_star,
                          log   = TRUE
    ) + dgamma(lambda_1,
               shape = y_1 + 2,
               rate  = t_2 - t_1 + 1/beta_star,
               log   = TRUE
    ) - dgamma(lambda_0,
               shape = y_0 + 2,
               rate  = t_1 - t_0 + 1/beta,
               log   = TRUE
    ) - dgamma(lambda_1,
               shape = y_1 + 2,
               rate  = t_2 - t_1 + 1/beta,
               log   = TRUE
    )
    alpha <- min(1, exp(ltarget_ratio + lprop_ratio))
    if (runif(1) < alpha) {
      # Accept proposals
      beta <- beta_star
      lambda_0 <- lambda_0_star
      lambda_1 <- lambda_1_star
      accept <- 1
    }
  }
  list(beta = beta, lambda_0 = lambda_0, lambda_1 = lambda_1, accept = accept)
}

# Samples from the posterior distribution using block McMC
# data:     number of observations
# t_1:      initial changepoint
# lambda_0: initial rateparameter
# lambda_1: initial rateparameter
# beta:     initial hyperparameter
# d:        tuning parameter for the proposal distribution
# n_iter:   number of iterations
block_McMC <- function(data, t_1, lambda_0, lambda_1, beta, d1 = 1, d2 = 1, n_iter = 5000)
  {
  x <- data$date
  y <- length(x)  # Number of disasters
  t_0 <- x[1]     # Start date
  t_2 <- x[y]     # End date
  theta <- matrix(nrow = n_iter, ncol = 4,
                  dimnames = list(NULL, c("t_1", "lambda_0", "lambda_1", "beta"))
  )
  theta[1, ] <- c(t_1, lambda_0, lambda_1, beta)
  y_0 <- sum(x < t_1)  # Number of disasters before t_1
  y_1 <- y - y_0       # Number of disasters after t_1
  accept_count_1 <- 0
  accept_count_2 <- 0
  if (n_iter > 1) {
    for (i in 2:n_iter) {
      if (i %% 2 == 0) {
        # Block proposal for (t_1, lambda_0, lambda_1) keeping beta unchanged
        param <- MH_t1_lamb0_lamb1(
                   t_1, lambda_0, lambda_1, beta, y_0, y_1, y, x, t_0, t_2, d1
                 )
        t_1 <- param$t_1
        y_0 <- param$y_0
        y_1 <- param$y_1
        lambda_0 <- param$lambda_0
        lambda_1 <- param$lambda_1
        accept_count_1 <- accept_count_1 + param$accept
      } else {
        # Block proposal for (beta, lambda_0, lambda_1) keeping t_0 unchanged
        param2 <- MH_beta_lamb0_lamb1(
                    t_1, lambda_0, lambda_1, beta, y_0, y_1, t_0, t_2, d2
                  )
        beta <- param2$beta
        lambda_0 <- param2$lambda_0
        lambda_1 <- param2$lambda_1
        accept_count_2 <- accept_count_2 + param2$accept
      }
      theta[i, ] <- c(t_1, lambda_0, lambda_1, beta)  # Update
    }
  }
  print(sprintf("Acceptance rate for (t_1, lambda_0, lambda_1): %.2f%%",
                2*100*accept_count_1/(n_iter - 1)))
  print(sprintf("Acceptance rate for (beta, lambda_0, lambda_1): %.2f%%",
                2*100*accept_count_2/(n_iter - 1)))
  as.data.frame(theta)
}
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 8.

We sample three chains using the block McMC by keeping the tuning parameter ofwith three different values for the tuning parameter from the variance of the Normal proposal of $t_1$.
The values are $d=1,10,200$.
The result is seen in Figure \ref{fig:A-block1}.
The chains with the tuning parameter $d=10$ and $200$ mixes almost instantly, while the chain with tuning parameter $d=1$ does not mix until after $n=4000$ steps.
```{r block1 sample, fig.cap="\\label{fig:A-block1}Trace plots of three chains with tuning parameters $d=1, 10$ and $200$ using block proposal for $(t_1, \\lambda_0, \\lambda_1)$."}
set.seed(93)
n_iter <- 5000
d <- c(1, 10, 200)

# Sample 3 chains with different values for tuning parameter

theta5 <- block_McMC(coal, coal$date[nrow(coal)], 10, 10, 10, d[1], d[1], n_iter)
theta6 <- block_McMC(coal, coal$date[nrow(coal)], 10, 10, 10, d[2], d[1], n_iter)
theta7 <- block_McMC(coal, coal$date[nrow(coal)], 10, 10, 10, d[3], d[1], n_iter)

# Plot trace
plot_trace(theta5, theta6, theta7) + scale_color_hue(name = "d_t", labels = d)
```

Figure \ref{fig:A-block1-burnin} shows the same trace plot but after discarding the burn-in.
We see that the chain with $d=200$ yields a poor mixing due to the proposal values constantly getting rejected.
The chain with $d=1$ is moving slowly in $t_1$, but performs better on the other parameters.
The chain with $d=10$ shows a combination of the patterns of the other two chains.

```{r block1 burnin, fig.cap="\\label{fig:A-block1-burnin}Trace plots of three chains with tuning parameters $d=1, 10$ and $200$ using block proposal for $(t_1, \\lambda_0, \\lambda_1)$ after discarding the burn-in."}
burnin <- 4000
theta5 <- tail(theta5, -burnin)
theta6 <- tail(theta6, -burnin)
theta7 <- tail(theta7, -burnin)

# Plot trace
plot_trace(theta5, theta6, theta7) + scale_color_hue(name = "d_t", labels = d)
```

We use the single-site chain to estimate the marginal posterior distributions $f_{t_1\mid x}(t_1\mid x)$, $f_{\lambda_0\mid x}(\lambda_0\mid x)$, $f_{\lambda_1\mid x}(\lambda_1\mid x)$ and $f_{\beta\mid x}(\beta\mid x)$ and their expected values.
The result is seen in Figure \ref{fig:theta-marginal}, showing the most interesting result of $\E[t_1\mid x] = 1890.61$, which coincides with our expectation.
The expected rates before and after $\E[t_1\mid x]$ shows that the rate decreases by $2.2$ disasters per year, which is a huge improvement(although not ideal) in the safety of the coal-miners.


```{r}
set.seed(93)

# Sample 3 chains with different values for tuning parameter d_beta
theta8 <- block_McMC(coal, coal$date[nrow(coal)], 10, 10, 10, d[2], d[1], n_iter)
theta9 <- block_McMC(coal, coal$date[nrow(coal)], 10, 10, 10, d[2], d[2], n_iter)
theta10 <- block_McMC(coal, coal$date[nrow(coal)], 10, 10, 10, d[2], d[3], n_iter)

# Plot trace
plot_trace(theta8, theta9, theta10) + scale_color_hue(name = "d_beta", labels = d)
```

```{r}
burnin <- 4000
theta8 <- tail(theta8, -burnin)
theta9 <- tail(theta9, -burnin)
theta10 <- tail(theta10, -burnin)

# Plot trace
plot_trace(theta8, theta9, theta10) + scale_color_hue(name = "d_beta", labels = d)
```

```{r theta marginal densities, fig.cap="\\label{fig:theta-marginal}Estimated marginal posterior distributions from single-site McMC, with the empirical mean."}
theta1 %>% 
  pivot_longer(everything(), names_to = "parameter") %>% 
  group_by(parameter) %>% 
  mutate(Expectation = mean(value)) %>% 
  ggplot(aes(value)) +
  geom_density() +
  geom_vline(aes(xintercept = Expectation, color = factor(round(Expectation, 2)))) +
  facet_wrap(~ parameter, nrow = 2, scales = "free") +
  labs(color = "Expectation") +
  theme_minimal()
```

The scatter plot of $[\lambda_0,\lambda_1\mid x]$ is shown in Figure \ref{fig:A-cov-lamb} also showing $\Cov[\lambda_0,\lambda_1\mid x]$. Since we are condition It is very low, and there is shows a small correlation.
```{r, fig.width=3.5, fig.asp=1, fig.cap="\\label{fig:A-cov-lamb}Scatter plot of $[\\lambda_0 \\mid x]$ and $[\\lambda_1 \\mid x]$ with their respective covariance."}
theta1 %>% 
  ggplot(aes(lambda_0, lambda_1)) +
  geom_point() +
  labs(
    subtitle = sprintf("Cov(lambda_0, lambda_1 | x) = %.4f",
                       with(theta1, cov(lambda_0, lambda_1)))
  ) +
  theme_minimal()
```



[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------


# Problem B: INLA for Gaussian Data
We plot the dataset in the following code block, and the result is shown in Figure \ref{fig:TimesSeriesOfGaussianData}.

```{r Plot Gaussian data, fig.width = 4, fig.height = 2, fig.cap = "\\label{fig:TimesSeriesOfGaussianData}The dataset \\texttt{Gaussiandata.txt} plotted."}
# Importing the data and making relevant variables:
path <- "https://www.math.ntnu.no/emner/TMA4300/2020v/Exercise/exercise2/Gaussiandata.txt"
gauss_data <- read.delim(path, header = FALSE)  # There is no header in the dataset
y <- gauss_data$V1    # Data values
TT <- length(y)       # End point of the data
t <- seq(1, TT, 1)    # A sequence from 1 to 20 for plotting

# Plotting:
ggplot(mapping = aes(t, y)) +
  geom_point() +
  ggtitle("Plot of the Gaussian data") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 1.
We assume that, given $\bfeta = \begin{bmatrix} \eta_1 & \cdots & \eta_T \end{bmatrix}^\top$, the observations $y_t$ of the time series are independent and Gaussian distributed with mean $\eta_t$ and known unit variance. Furthermore, the linear predictor $\eta_t$ is linked to a smooth effect of time $t$ as $\eta_t = f_t$, where the vector $\bff = \begin{bmatrix} f_1 & \cdots & f_T \end{bmatrix}^\top$ is the latent field. Choosing a second order random walk model as a prior distribution for $\bff$ we get that $\bff \mid \theta \sim \Normal(\bfzero, Q(\theta)^{-1})$, for some precision matrix $Q(\theta)$. The model is completed by assigning the prior $\theta \sim \Gammadist(1, 1)$.

The class of latent Gaussian models can be represented by a hierarchical structure containing three stages. In the case of this model it can be written in the stages
$$
  \bfy \mid \bff \sim \prod_{t=1}^T \pi(y_t \mid \eta_t),
  \quad
  \bff \mid \theta \sim \Normal(\bfzero, Q(\theta)^{-1})
  \quad \text{and} \quad
  \theta \sim \Gammadist(1, 1),
$$
where $Q(\theta)$ is the precision matrix, and $y_t \mid \eta_t \sim \Normal(\eta_t, 1)$, for $t = 1, \dots, T$. The field $\bff$ is also a Gaussian Markov random field, as can be seen from its distribution and the sparseness discussed below. Thus, because the model here can be written in the above form, it is a latent Gaussian model.

INLA is applicable for use in latent Gaussian models under some constraints. Firstly, the hyperparameter vector cannot be to large, and in this model it is just a scalar, so this is satisfied. Secondly, the precision matrix needs to be sparse, which is the case for a second order random walk model. We look further at
$$
  \pi(\bff \mid \theta) \propto \theta^{(T-2)/2} \exp\left[ -\frac{\theta}{2} \sum_{t=3}^T (f_t - 2 f_{t-1} + f_{t-2})^2 \right] = \theta^{(T-2)/2} \exp\left[ -\frac{1}{2} \bff^\top Q(\theta) \bff \right],
$$
and want to determine $Q(\theta)$, so we look more at the exponent. We get that
$$
  \theta \sum_{t=3}^T (f_t - 2 f_{t-1} + f_{t-2})^2 = \theta \sum_{t=3}^T (f_t^2 + 4 f_{t-1}^2 + f_{t-2}^2 - 4 f_t f_{t-1} + 2 f_t f_{t-2} - 4 f_{t-1} f_{t-2}) = \bff^\top Q(\theta) \bff,
$$
with the precision matrix
$$
  Q(\theta) = \theta
  \begin{bmatrix}
    1 & -2 & 1 &  &  &  &  &  & 
     \\
    -2 & 5 & -4 & 1 &  &  &  &  & 
     \\
    1 & -4 & 6 & -4 & 1 &  &  &  & 
     \\
     & 1 & -4 & 6 & -4 & 1 &  &  & 
     \\
     &  & \ddots & \ddots & \ddots & \ddots & \ddots &  & 
     \\
     &  &  & 1 & -4 & 6 & -4 & 1 & 
     \\
     &  &  &  & 1 & -4 & 6 & -4 & 1
     \\
     &  &  &  &  & 1 & -4 & 5 & -2
     \\
     &  &  &  &  &  & 1 & -2 & 1
     \\
  \end{bmatrix},
$$
where the elements not filled in are zero, that is, a sparse matrix. There is also a multiplication, such that all elements gets a factor $\theta$.

All the criteria to be able to use INLA is thus satisfied, and it can be applied to the model.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.
We now want to define and implement a block Gibbs sampling algorithm for $f(\bfeta, \theta \mid \bfy)$ by proposing a new value for $\theta$ from the full conditional $\pi(\theta \mid \bfeta, \bfy)$, and proposing a new value for $\bfeta$ from the full conditional $\pi(\bfeta \mid \theta, \bfy)$. Noting that $\bff = \bfeta$, we firstly have that
\begin{align*}
  f(\bfeta, \theta \mid \bfy) &\propto \pi(\bfy \mid \bfeta, \theta) \pi(\bfeta \mid \theta) \pi(\theta)
  \\
  &\propto \left\{ \prod_{t=1}^T \exp\left[ -\frac{1}{2} (y_t - \eta_t)^2 \right] \right\} \left\{ \theta^{(T-2)/2} \exp\left[ -\frac{\theta}{2} \sum_{t=3}^T (\eta_t - 2 \eta_{t-1} + \eta_{t-2})^2 \right] \right\} \exp(-\theta)
  \\
  &= \theta^{(T-2)/2} \exp\left[ -\theta - \frac{\theta}{2} \sum_{t=3}^T (\eta_t - 2 \eta_{t-1} + \eta_{t-2})^2 - \frac{1}{2} \sum_{t=1}^T (y_t - \eta_t)^2 \right],
\end{align*}
and we can from this find the full conditionals. This gives us
$$
  \pi(\theta \mid \bfeta, \bfy) \propto \theta^{(T-2)/2} \exp\left[ -\theta - \frac{\theta}{2} \sum_{t=3}^T (\eta_t - 2 \eta_{t-1} + \eta_{t-2})^2 \right] = \theta^{(T-2)/2} \exp\left[ -\theta - \frac{\theta}{2} \bfeta^\top Q(1) \bfeta \right],
$$
and we see that
$$
  \theta \mid \bfeta, \bfy \sim \Gammadist\left( \frac{T}{2}, 1 + \frac{1}{2} \bfeta^\top Q(1) \bfeta \right),
$$
where the gamma distribution is parameterized with the shape and rate parameter. We can also find
\begin{align*}
  \pi(\bfeta \mid \theta, \bfy) &\propto \exp\left[ -\frac{\theta}{2} \sum_{t=3}^T (\eta_t - 2 \eta_{t-1} + \eta_{t-2})^2 - \frac{1}{2} \sum_{t=1}^T (y_t - \eta_t)^2 \right]
  \\
  &= \exp\left[ -\frac{1}{2} \bfeta^\top Q(\theta) \bfeta - \frac{1}{2} (\bfy - \bfeta)^\top (\bfy - \bfeta) \right]
  \\
  &= \exp\left[ -\frac{1}{2} (\bfeta^\top Q(\theta) \bfeta + \bfeta^\top \bfeta + \bfy^\top \bfy) + \bfy^\top \bfeta \right]
  \\
  &\propto \exp\left[ \bfy^\top \bfeta - \frac{1}{2} \bfeta^\top (Q(\theta) + I) \bfeta \right],
\end{align*}
where $I \in \RR^{T \times T}$ is the identity matrix. It is then clear that
$$
  \bfeta \mid \theta, \bfy \sim \Normal((Q(\theta) + I)^{-1} \bfy, (Q(\theta) + I)^{-1}),
$$
because we recognize the canonical form of the Gaussian.

We can then create the matrix $Q(\theta)$ as `Q(TT, theta)` in `R`, done in the following code block. Here we use that $Q(\theta) = \theta M^\top M$ for the matrix $M$ coming from the fact that we have $\sum_{t=3}^T (f_t - 2f_{t-1} + f_{t-2})^2$.

```{r Define Q matrix}
Q_matrix <- function(TT, theta) {
  M <- matrix(0, nrow = TT, ncol = TT)  # Matrix of zeros of size TT times TT
  for (i in 3:TT) { # Create M corresponding to RW2
    M[i, c(i-2, i-1, i)] <- c(1, -2, 1)
  }
  return(theta * t(M) %*% M)  # Q(theta) = theta * M^T * M
}
```

We can then create a function to sample using the block updating using Gibbs sampling. This is done in the following code block.

```{r Block Gibbs sampler}
block_Gibbs <- function(n, theta0, eta0, y) {
  TT <- length(eta0)
  theta <- rep(0, n)  # Initialize vector to store theta
  eta <- matrix(0, nrow = n, ncol = TT)  # Initialize matrix to store eta (row-wise)
  theta[1] <- theta0  # Initial value
  eta[1, ] <- eta0    # Initial value
  Q1 <- Q_matrix(TT, 1)  # Q(1)
  for (i in 2:n) {
    # Using old eta to find new theta from gamma distribution:
    theta[i] <- rgamma(1, shape = TT/2, rate = 1 + 1/2 * eta[i-1, ] %*% Q1 %*% eta[i-1, ])
    Qt <- Q_matrix(TT, theta[i])   # Using new theta to find Q(theta)
    inv <- solve(Qt + diag(TT))    # (Q(theta) + I)^(-1)
    eta[i, ] <- rmvnorm(1, inv %*% y, inv)  # Drawing from multivariate normal
  }
  return(list("theta" = theta, "eta" = eta))
}
```

Now we may run the implementation done in the following code block. Here we initialize $\theta = 1$ and $\bfeta = \begin{bmatrix} 1 & \cdots & 1 \end{bmatrix}^\top$.

```{r Testing block Gibbs sampler, fig.cap = "\\label{fig:TraceplotTheta}The traceplot of $\\theta$ giving an indication of the burn-in period."}
set.seed(420)
n <- 5000
theta0 <- 1         # Initial value
eta0 <- rep(1, TT)  # Initial value

Gibbs <- block_Gibbs(n, theta0, eta0, y)
theta <- Gibbs$theta    # theta values from Gibbs sampling

# Plotting:
as.data.frame(theta) %>%
  ggplot(aes(x = seq(1, n, 1), y = theta)) +
  geom_line() +
  ggtitle("Traceplot of theta") +
  xlab("Iteration") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

From Figure \ref{fig:TraceplotTheta} the burn-in period seems to be no more than 50, but to be conservative, we disregard the 100 first iterations. Knowing this, we discard the first 100 iterations. This is done in the following code block.

```{r Block Gibbs sampler inference, fig.cap = "\\label{fig:HistogramOfTheThetaSamples}Histogram of the $\\theta$ samples."}
thetaGibbs <- tail(theta, -100)  # Not containing the first 100 values

# Plotting:
as.data.frame(thetaGibbs) %>%
  ggplot(aes(x = thetaGibbs, y = ..density..)) +
  geom_histogram(binwidth = 0.1, boundary = 0) +
  ggtitle("Histogram of the theta samples") +
  xlab("theta") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Figure \ref{fig:HistogramOfTheThetaSamples} shows a simulated histogram of the $\theta$ samples from the block Gibbs sampler. This is an approximation of the posterior marginal for the hyperparameter, $\pi(\theta \mid \bfy)$.

We can now calculate the mean and 95% confidence interval of the smooth effect. This is done in the following code block, and the result is shown in Figure \ref{fig:ObservedDataWithMeanAndCI}. From this we see that almost all the data points are within the confidence interval.

```{r Mean and CI of smooth effect, fig.cap = "\\label{fig:ObservedDataWithMeanAndCI}The mean and the 95% confidence interval for the smooth effect."}
eta <- tail(Gibbs$eta, -100)  # eta values from Gibbs sampler omitting the first 100
eta_mean <- colMeans(eta) # Column-wise mean of eta
eta_var <- colVars(eta)   # Column-wise variance of eta

z <- qnorm(0.025)   # z-value for 95% confidence interval
CI_upp <- eta_mean + z * sqrt(eta_var)  # Upper bound of confidence interval
CI_low <- eta_mean - z * sqrt(eta_var)  # Lower bound of confidence interval

# Plotting:
ggplot() +
  geom_line(aes(x = t,
                y = eta_mean,
                col = "Mean")
            ) +
  geom_point(aes(x = t,
                 y = y,
                 col = "Observed data")
             ) +
  geom_line(aes(x = t,
                y = CI_upp,
                col = "95% confidence interval")
            ) +
  geom_line(aes(x = t,
                y = CI_low,
                col = "95% confidence interval")
            ) +
  geom_ribbon(aes(x = t,
                  y = y,
                  ymin = CI_low,
                  ymax = CI_upp),
              fill = "red",
              alpha = 0.05
              ) +
  ylab("y") +
  ggtitle("Mean and confidence interval for the smooth effect") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.
We now want to approximate the posterior marginal for the hyperparameter using INLA. We have that
$$
  \pi(\theta \mid \bfy) \propto \frac{\pi(\bfy \mid \bfeta, \theta) \pi(\bfeta \mid \theta) \pi(\theta)}{\pi(\bfeta \mid \theta, \bfy)},
$$
where we previously found that $\bfeta \mid \theta, \bfy \sim \Normal((Q(\theta) + I)^{-1} \bfy, (Q(\theta) + I)^{-1})$, and also the other quantities are known up to a proportionality constant. We thus have that
\begin{align*}
  \pi(\theta \mid \bfy) &\propto \frac{\exp\left[ -\frac{1}{2} (\bfeta - \bfy)^\top (\bfeta - \bfy) \right] \theta^{(T-2)/2} \exp\left[  -\frac{1}{2} \bfeta^\top Q(\theta) \bfeta \right] \exp(-\theta)}{\det(Q(\theta) + I)^{1/2} \exp\left\{ -\frac{1}{2} [\bfeta - (Q(\theta) + I)^{-1} \bfy]^\top (Q(\theta) + I) [\bfeta - (Q(\theta) + I)^{-1} \bfy] \right\}}
  \\
  &= \theta^{(T-2)/2} \det(Q(\theta) + I)^{-1/2}
  \\
  &\phantom{=} \cdot \exp\left\{ -\theta - \frac{1}{2} (\bfeta - \bfy)^\top (\bfeta - \bfy) - \frac{1}{2} \bfeta^\top Q(\theta) \bfeta + \frac{1}{2} [\bfeta - (Q(\theta) + I)^{-1} \bfy]^\top (Q(\theta) + I) [\bfeta - (Q(\theta) + I)^{-1} \bfy] \right\}
  \\
  &\propto \theta^{(T-2)/2} \det(Q(\theta) + I)^{-1/2} \exp\left[ -\theta - \frac{1}{2} \bfy^\top \bfy + \frac{1}{2} \bfy^\top (Q(\theta) + I)^{-1} \bfy \right]
  \\
  &= \theta^{(T-2)/2} \det(Q(\theta) + I)^{-1/2} \exp\left\{ -\theta - \frac{1}{2} \bfy^\top [I - (Q(\theta) + I)^{-1}] \bfy \right\},
\end{align*}
where we have used that $(Q(\theta) + I)^\top = Q(\theta)^\top + I^\top = Q(\theta) + I$, because $Q(\theta)$ is symmetric. We can then based on this simulate the posterior marginal for the hyperparameter as done in the following code block.

```{r Posterior marginal}
post_marg_theta <- function(thetas, y) {
  post <- rep(0, length(thetas))
  TT <- length(y)
  for (i in 1:length(post)) {
    theta <- thetas[i]
    Q <- Q_matrix(TT, theta)
    QpI <- Q + diag(TT)
    post[i] <- theta^(TT/2-1) * det(QpI)^(-1/2) *
               exp(-theta - 1/2 * t(y) %*% (diag(TT) - solve(QpI)) %*% y)
  }
  return(post)
}
```

To test this we make a grid of $\theta$'s from 0 to 6 and find $\pi(\theta \mid \bfy)$ on this grid. We could just plot this and see that the form is the same as the one we obtained in Figure \ref{fig:HistogramOfTheThetaSamples}. However, for this simple example it is easy to find the normalizing constant by integration. Doing this, and plotting together with the previously found histogram, we get Figure \ref{fig:PosteriorMarginal}. We see that there is a strong correspondence between the two.

```{r Plot posterior marginal, fig.cap = "\\label{fig:PosteriorMarginal}Posterior marginal for the hyperparameter both using block Gibbs sampling and INLA."}
set.seed(420)

# Using INLA scheme:
thetas <- seq(0, 6, 0.01)   # Grid of thetas
post <- post_marg_theta(thetas, y)  # Un-normalized posterior marginal

k <- integrate(function(theta)(post_marg_theta(theta, y)),
               lower = 0,
               upper = Inf)$value
post <- post / k    # Normalized posterior marginal

# Plotting:
ggplot() +
  geom_histogram(aes(x = thetaGibbs,
                     y = ..density..),
                 binwidth = 0.1,
                 boundary = 0
                 ) +
  geom_line(aes(x = thetas,
                y = post,
                col = "Using INLA\nscheme")
            ) +
  xlab("theta") +
  ylab("pi(theta | y)") +
  ggtitle("Posterior marginal for the hyperparameter") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 4.
In this problem we want to approximate the marginal posterior of the smooth effect given by
$$
  \pi(\eta_t \mid \bfy) = \int_\RR \pi(\eta_t \mid \bfy, \theta) \pi(\theta \mid \bfy) \dtheta,
$$
for $t = 1, \dots, T$. We are interested in the marginal posterior of the smooth effect for $t = 10$, that is $\pi(\eta_{10} \mid \bfy)$, and we approximate this as
$$
  \pi(\eta_{10} \mid \bfy) \approx \Delta \sum_{\theta \in \Theta} \pi(\eta_{10} \mid \bfy, \theta) \pi(\theta \mid \bfy).
$$
Here we denote the grid of $\theta$ values by $\Theta$, and the step size, which is in our case equal for each grid-point, by $\Delta$.

In the following code block we first make functions for $\pi(\eta_t \mid \bfy, \theta)$ and $\pi(\eta_t \mid \bfy)$, before we make grids and plot the result. We plot it together with the result from the block Gibbs sampling, and again we normalize the result from the INLA scheme, as it is simple to do in our case. This is shown in Figure \ref{fig:PosteriorMarginalSmoothEffect}, and we see that the two posterior marginals correspond well.

```{r Posterior marginal smooth effect, fig.cap = "\\label{fig:PosteriorMarginalSmoothEffect}The posterior marginal for the smooth effect using the INLA scheme and using block Gibbs sampling."}
# Using INLA scheme:
pi_eta_given_theta_y <- function(y, theta, etas, t) {   # pi(eta_t | theta, y)
  TT <- length(y)
  Q <- Q_matrix(TT, theta)
  inv <- solve(Q + diag(TT))
  # eta_t | theta, y ~ Normal(inv * y, inv):
  normal <- dnorm(etas, mean = (inv %*% y)[t], sd = sqrt(inv[t, t]))
  return(normal)
}

post_marg_eta <- function(y, thetas, etas, t) {   # pi(eta_t | y)
  res <- rep(0, length(etas))   # Initializing result to zero
  Delta <- thetas[2] - thetas[1]  # Step size
  post_theta <- post_marg_theta(thetas, y)  # pi(theta | y)
  for (j in 1:length(thetas)) {   # Approximation of the integral
    res <- res + Delta * pi_eta_given_theta_y(y, thetas[j], etas, t) * post_theta[j]
  }
  return(res)
}

# Un-normalized:
etas <- seq(-3, 3, 0.01)    # Grid of etas
thetas <- seq(0, 6, 0.01)   # Grid of thetas
eta10 <- post_marg_eta(y, thetas, etas, 10)

# Normalizing:
k <- integrate(function(etas)(post_marg_eta(y, thetas, etas, 10)),
               lower = -Inf,
               upper = Inf)$value
eta10 <- eta10 / k

# From block Gibbs sampling:
eta10Gibbs <- eta[, 10]

# Plotting:
ggplot() +
  geom_histogram(mapping = aes(x = eta10Gibbs, y = ..density..),
                 binwidth = 0.1,
                 boundary = 0
                 ) +
  geom_line(aes(x = etas,
                y = eta10,
                col = "Using INLA\nscheme")
            ) +
  xlab("eta10") +
  ylab("pi(eta10 | y)") +
  ggtitle("Marginal posterior for the smooth effect for t = 10") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 5.
We are now going to use the `R-INLA` library to do what we have done previously in this problem. We first perform the calculations using the `inla()` function in the following code block.

```{r INLA}
df <- data.frame(y = y, t = t)
hyper <- list(theta = list(prior = "loggamma", param = c(1, 1)))
formula <- y ~ f(t, model = "rw2", hyper = hyper, constr = FALSE) - 1
res <- inla(formula = formula,
            family = "gaussian",
            data = df,
            control.family = list(hyper = list(prec = list(initial = 0, fixed = TRUE))))
```

Firstly, we may then show the posterior marginal for the hyperparameter, as shown in Figure \ref{fig:INLAPosteriorMarginalTheta}. We see that there indeed is a close correspondence between the methods used.

```{r INLA posterior marginal theta, fig.cap = "\\label{fig:INLAPosteriorMarginalTheta}The posterior marginal for the hyperparameter. It shows the results of the block Gibbs sampling, the INLA scheme, and the `R-INLA` package."}
ggplot() +
  geom_histogram(aes(x = thetaGibbs,
                     y = ..density..),
                 binwidth = 0.1,
                 boundary = 0
                 ) +
  geom_line(aes(x = res$marginals.hyperpar$`Precision for t`[, 1],
                y = res$marginals.hyperpar$`Precision for t`[, 2],
                col = "Using R-INLA")
            ) +
  geom_line(aes(x = thetas,
                y = post,
                col = "Using INLA\nscheme"),
            linetype = "dashed"
            ) +
  xlim(0, 7) +
  ggtitle("Posterior marginal for the hyperparameter") +
  xlab("theta") +
  ylab("pi(theta | y)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Secondly, we plot the mean of the smooth effect, as shown in Figure \ref{fig:INLAMeanOfSmoothEffect}. Also here we see a strong correspondence.

```{r INLA mean smooth effect, fig.cap = "\\label{fig:INLAMeanOfSmoothEffect}The mean of the smooth effect. It shows the result using block Gibbs sampling and the `R-INLA` package."}
ggplot() +
  geom_line(aes(x = t,
                y = res$summary.random$t$mean,
                col = "Using R-INLA")
            ) +
  geom_line(aes(x = t,
                y = eta_mean,
                col = "Using block Gibbs"),
            linetype = "dashed"
            ) +
  ylab("y") +
  ggtitle("Mean for the smooth effect") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Lastly, we may use the result to plot the posterior marginal for the smooth effect, as shown in Figure \ref{fig:INLAPosteriorMarginalEta}. Finally we here see a good correspondence with the methods we used.

```{r INLA posterior marginal eta10, fig.cap = "\\label{fig:INLAPosteriorMarginalEta}The posterior marginal for the smooth effect for $t=10$. It shows the results of the block Gibbs sampling, the INLA scheme, and the `R-INLA` package."}
ggplot() +
  geom_histogram(mapping = aes(x = eta10Gibbs, y = ..density..),
                 binwidth = 0.1,
                 boundary = 0
                 ) +
  geom_line(aes(x = res$marginals.random$t$index.10[, 1],
                y = res$marginals.random$t$index.10[, 2],
                col = "Using R-INLA")
            ) +
  geom_line(aes(x = etas,
                y = eta10,
                col = "Using INLA\nscheme"),
            linetype = "dashed"
            ) +
  xlim(-3, 3) +
  xlab("eta10") +
  ylab("pi(eta10 | y)") +
  ggtitle("Marginal posterior for the smooth effect for t = 10") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------