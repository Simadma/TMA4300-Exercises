---
title: "Exercise 1"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE,
                      strip.white = TRUE, prompt = FALSE, cache = TRUE,
                      size = "scriptsize", fig.width = 6, fig.height = 4)
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}


```{r libraries, eval = TRUE, echo = FALSE}
library(ggplot2)  # Create Elegant Data Visualizations Using the Grammar of Graphics
#library(ISLR)  # Data from the book ISLR
#library(boot)  # Bootstrap Functions
#library(MASS)  # Support Functions and Datasets for Venables and Ripley's MASS
#library(FactoMineR)  # Multivariate Exploratory Data Analysis and Data Mining
#library(factoextra)  # Extract and Visualize the Results of Multivariate Data Analyses
#library(ggfortify)  # Data Visualization Tools for Statistical Analysis Results
#library(glmnet)  # Lasso and Elastic-Net Regularized Generalized Linear Models
#library(tree)  # Classification and Regression Trees
#library(randomForest)  # Breiman and Cutler's Random Forests for Classif. and Regress.
#library(gbm)  # Generalized Boosted Regression Models
#library(keras)  # R Interface to 'Keras'
#library(pls)  # Partial Least Squares and Principal Component Regression
#library(gam)  # Generalized Additive Models
#library(gridExtra)  # Mescellaneous Functions for "Grid" Graphics
#library(GGally)  # Extension to 'ggplot2'
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## 1.
Let $X \sim \Exp(\lambda)$, with the cdf
$$
  F_X(x) = 1 - e^{-\lambda x},\quad x \geq 0.
$$
Then the random variable $Y := F_X(X)$ has a $\Uniform(0, 1)$ distribution. The probability integral transform becomes

\begin{equation}\label{eq:exp_transf}
  Y = 1 - e^{-\lambda X} \Leftrightarrow X = -\frac{1}{\lambda}\log(1 - Y).
\end{equation}

Thus, we sample $Y$ from `runif()` and transform it using \eqref{eq:exp_transf}, to sample from the exponential distribution. Figure \ref{fig:exp_hist} shows one million samples drawn from the `generate_from_exp()` function defined in the code chunk below.

```{r generate from exp, fig.cap = "\\label{fig:exp_hist}Normalized histogram of one million samples drawn from the exponential distribution, together with the theoretical pdf, with $\\lambda = 4.32$."}
set.seed(123)

generate_from_exp <- function(n, rate = 1) {
  Y <- runif(n)
  X <- -(1 / rate) * log(1 - Y)
  X
}

# sample
n <- 1000000  # One million samples
lambda <- 4.32
x <- generate_from_exp(n, rate = lambda)

# plot
hist(x,
  breaks      = 80,
  probability = TRUE,
  xlim        = c(0, 2)
)
curve(dexp(x, rate = lambda),
  add = TRUE,
  lwd = 2,
  col = "red"
)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)

### (c)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5

[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem B: The gamma distribution

## 1.

### (a)

Let $f(x)$ be the target distribution we wish to sample from, and let $g(x)$ be the proposal distribution. For the rejection sampling algorithm, we require that
\begin{equation}\label{eq:rej_samp}
  f(x) \leq c\cdot g(x),\quad\forall x\in\mathbb R,
\end{equation}
for some constant $c>0$. Let $X$ and $U$ be independent samples where $X \sim g(x)$ and $U \sim \Uniform(0, 1)$. Then the acceptance probability is
$$
\begin{split}
  \Pr\left(U\leq\frac{f(X)}{c\cdot g(X)}\right) &= \int_{-\infty}^\infty\int_{0}^{f(x)/(c\ g(x))}f_{X,U}(x, u)\,du\,dx \\
  &= \int_{-\infty}^\infty\int_{0}^{f(x)/(c\ g(x))}g(x)\cdot 1\,du\,dx \\
  &= \int_{-\infty}^\infty\frac{f(x)}{c\ g(x)}g(x)\,dx \\
  &= \frac{1}{c}\int_{-\infty}^\infty f(x)\,dx \\
  &= \frac{1}{c}
\end{split}
$$

We wish to sample from $\operatorname{Gamma}(\alpha, \beta = 1)$, using the proposal distribution $g(x)$ given in `eqref{??????}`. We want to choose $c$ such that the acceptance probability is maximized while \eqref{eq:rej_samp} is satisfied. We must check three cases. The trivial case when $x\leq 0$, we have $f(x) = g(x) = 0$ so \eqref{eq:rej_samp} is satisfied for all $c$. When $0<x<1$ we have

$$
\begin{split}
  f(x) &\leq c\,g(x) \\
  \frac{1}{\Gamma(\alpha)}x^{\alpha - 1}e^{-x} &\leq c\,\frac{1}{\alpha^{-1} + e^{-1}}x^{\alpha - 1} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}e^{-x} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}.
\end{split}
$$

The last case, when $x\geq 1$, we have
$$
\begin{split}
  f(x) &\leq c\,g(x) \\
  \frac{1}{\Gamma(\alpha)}x^{\alpha - 1}e^{-x} &\leq c\,\frac{1}{\alpha^{-1} + e^{-1}}e^{-x} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}x^{\alpha - 1} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}.
\end{split}
$$
That is, we choose $c := (\alpha^{-1} + e^{-1})/\Gamma(\alpha)$, such that the acceptance probability becomes
$$
\Pr\left(U\leq\frac{f(X)}{c\cdot g(X)}\right) = \frac{1}{c} = \frac{\Gamma(\alpha)}{\alpha^{-1} + e^{-1}},\quad \alpha\in(0, 1).
$$


### (b)

```{r}
set.seed(137)

generate_from_gamma <- function(n, shape = 0.5) {
  c <- gamma(shape) / (1 / shape + 1 / exp(1))   # constant that minimizes the envelope
  x <- vector(mode = "numeric", length = n)
  for (i in 1:n) {
    repeat{
      x[i] <- generate_from_piecewise(1, shape)  # draw from proposal
      u <- runif(1)                              # draw from U(0, 1)
      f <- dgamma(x[i], shape = shape)           # target value
      g <- theo_PDF(x[i], alpha = shape)         # proposal value
      alpha <- (1 / c) * (f / g)
      if (u <= alpha) {
        break
      }
    }
  }
  return(x)
}

# n <- 1000000
# alpha <- 0.9
# x <- generate_from_gamma(n, shape = alpha)
# hist(x,
#   breaks      = 80,
#   probability = TRUE,
#   xlim        = c(0, 6)
# )
# curve(dgamma(x, shape = alpha),
#   add = TRUE,
#   lwd = 2,
#   col = "red"
# )
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------


# Problem C: Monte Carlo integration and variance reduction

## 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem D: Rejection sampling and importance sampling

## 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------