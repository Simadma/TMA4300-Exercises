---
title: "Exercise 1"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE,
                      strip.white = TRUE, prompt = FALSE, cache = TRUE,
                      size = "scriptsize", fig.width = 6, fig.height = 4)
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}


```{r libraries, eval = TRUE, echo = FALSE}
library(ggplot2)  # Create Elegant Data Visualizations Using the Grammar of Graphics
#library(ISLR)  # Data from the book ISLR
#library(boot)  # Bootstrap Functions
#library(MASS)  # Support Functions and Datasets for Venables and Ripley's MASS
#library(FactoMineR)  # Multivariate Exploratory Data Analysis and Data Mining
#library(factoextra)  # Extract and Visualize the Results of Multivariate Data Analyses
#library(ggfortify)  # Data Visualization Tools for Statistical Analysis Results
#library(glmnet)  # Lasso and Elastic-Net Regularized Generalized Linear Models
#library(tree)  # Classification and Regression Trees
#library(randomForest)  # Breiman and Cutler's Random Forests for Classif. and Regress.
#library(gbm)  # Generalized Boosted Regression Models
#library(keras)  # R Interface to 'Keras'
#library(pls)  # Partial Least Squares and Principal Component Regression
#library(gam)  # Generalized Additive Models
#library(gridExtra)  # Mescellaneous Functions for "Grid" Graphics
#library(GGally)  # Extension to 'ggplot2'
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## 1.
Let $X \sim \Exp(\lambda)$, with the cdf
$$
  F_X(x) = 1 - e^{-\lambda x},\quad x \geq 0.
$$
Then the random variable $Y := F_X(X)$ has a $\Uniform(0, 1)$ distribution. The probability integral transform becomes

\begin{equation}\label{eq:exp_transf}
  Y = 1 - e^{-\lambda X} \Leftrightarrow X = -\frac{1}{\lambda}\log(1 - Y).
\end{equation}

Thus, we sample $Y$ from `runif()` and transform it using \eqref{eq:exp_transf}, to sample from the exponential distribution. Figure \ref{fig:exp_hist} shows one million samples drawn from the `generate_from_exp()` function defined in the code chunk below.

```{r generate from exp, fig.cap = "\\label{fig:exp_hist}Normalized histogram of one million samples drawn from the exponential distribution, together with the theoretical pdf, with $\\lambda = 4.32$."}
set.seed(123)

generate_from_exp <- function(n, rate = 1) {
  Y <- runif(n)
  X <- -(1 / rate) * log(1 - Y)
  X
}

# sample
n <- 1000000  # One million samples
lambda <- 4.32
x <- generate_from_exp(n, rate = lambda)

# plot
hist(x,
  breaks      = 80,
  probability = TRUE,
  xlim        = c(0, 2)
)
curve(dexp(x, rate = lambda),
  add = TRUE,
  lwd = 2,
  col = "red"
)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)

### (c)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5

[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem B: The gamma distribution

## 1.

### (a)

Let $f(x)$ be the target distribution we wish to sample from, and let $g(x)$ be the proposal distribution. For the rejection sampling algorithm, we require that
\begin{equation}\label{eq:rej_samp}
  f(x) \leq c\cdot g(x),\quad\forall x\in\mathbb R,
\end{equation}
for some constant $c>0$. Let $X$ and $U$ be independent samples where $X \sim g(x)$ and $U \sim \Uniform(0, 1)$. Then the acceptance probability is
$$
\begin{split}
  \Pr\left(U\leq\frac{f(X)}{c\cdot g(X)}\right) &= \int_{-\infty}^\infty\int_{0}^{f(x)/(c\ g(x))}f_{X,U}(x, u)\,du\,dx \\
  &= \int_{-\infty}^\infty\int_{0}^{f(x)/(c\ g(x))}g(x)\cdot 1\,du\,dx \\
  &= \int_{-\infty}^\infty\frac{f(x)}{c\ g(x)}g(x)\,dx \\
  &= \frac{1}{c}\int_{-\infty}^\infty f(x)\,dx \\
  &= \frac{1}{c}
\end{split}
$$

We wish to sample from $\operatorname{Gamma}(\alpha, \beta = 1)$, using the proposal distribution $g(x)$ given in `eqref{??????}`. We want to choose $c$ such that the acceptance probability is maximized while \eqref{eq:rej_samp} is satisfied. We must check three cases. The trivial case when $x\leq 0$, we have $f(x) = g(x) = 0$ so \eqref{eq:rej_samp} is satisfied for all $c$. When $0<x<1$ we have

$$
\begin{split}
  f(x) &\leq c\,g(x) \\
  \frac{1}{\Gamma(\alpha)}x^{\alpha - 1}e^{-x} &\leq c\,\frac{1}{\alpha^{-1} + e^{-1}}x^{\alpha - 1} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}e^{-x} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}.
\end{split}
$$

The last case, when $x\geq 1$, we have
$$
\begin{split}
  f(x) &\leq c\,g(x) \\
  \frac{1}{\Gamma(\alpha)}x^{\alpha - 1}e^{-x} &\leq c\,\frac{1}{\alpha^{-1} + e^{-1}}e^{-x} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}x^{\alpha - 1} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}.
\end{split}
$$
That is, we choose $c := (\alpha^{-1} + e^{-1})/\Gamma(\alpha)$, such that the acceptance probability becomes
$$
\Pr\left(U\leq\frac{f(X)}{c\cdot g(X)}\right) = \frac{1}{c} = \frac{\Gamma(\alpha)}{\alpha^{-1} + e^{-1}},\quad \alpha\in(0, 1).
$$


### (b)

```{r gamma rejection sampling}
set.seed(137)

generate_from_gamma <- function(n, shape = 0.5) {
  c <- gamma(shape) / (1 / shape + 1 / exp(1))   # constant that minimizes the envelope
  x <- vector(mode = "numeric", length = n)
  for (i in 1:n) {
    repeat {
      x[i] <- generate_from_piecewise(1, shape)  # draw from proposal
      u <- runif(1)                              # draw from U(0, 1)
      f <- dgamma(x[i], shape = shape)           # target value
      g <- theo_PDF(x[i], alpha = shape)         # proposal value
      alpha <- (1 / c) * (f / g)
      if (u <= alpha) {
        break
      }
    }
  }
  return(x)
}

# n <- 1000000
# alpha <- 0.9
# x <- generate_from_gamma(n, shape = alpha)
# hist(x,
#   breaks      = 80,
#   probability = TRUE,
#   xlim        = c(0, 6)
# )
# curve(dgamma(x, shape = alpha),
#   add = TRUE,
#   lwd = 2,
#   col = "red"
# )
```



[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)
We will now use the ratio-of-uniforms method to simulate from $\operatorname{Gamma}(\alpha,\beta = 1)$. Additionally we have $\alpha>1$ this time. Let us define
\begin{equation}\label{eq:rat-of-uni}
  C_f = \left\{(x_1, x_2) : 0 \leq x_1 \leq \sqrt{f^*\left(\frac{x_2}{x_1}\right)}\right\}, \quad \text{where} \quad f^*(x) = \begin{cases}
           x^{\alpha - 1}e^{-x}, & x > 0,\\
           0, & \text{otherwise},
  \end{cases}
\end{equation}

and

\begin{equation}\label{eq:rat-bounds}
a = \sqrt{\sup_x f^*(x)},\quad b_+ = \sqrt{\sup_{x\geq 0}(x^2 f^*(x))}\quad \text{and}\quad b_- = -\sqrt{\sup_{x\leq 0}(x^2 f^*(x))},
\end{equation}

such that $C_f\subset[0,1]\times[b_-, b_+]$.

First we find $\sup_x f^*(x)$. This must be when $x>0$. We differentiate $f^*(x)$ and setting the expression equal to zero to find the stationary point.
$$
\begin{split}
  0 &= \frac{d}{dx} f^*(x) \\
  &= \frac{d}{dx} x^{\alpha - 1}e^{-x} \\
  &= e^{-x}x^{\alpha - 2}\left((\alpha - 1) - x\right) \\
  \Rightarrow\quad x &= \alpha - 1,\quad \text{where}\quad\alpha > 1.
\end{split}
$$
Since we have only one stationary point, $f^*(x)$ is continuous, $f^*(x) > 0\ \forall x>0$ and $\lim_{x\to0+}f^*(x) = \lim_{x\to\infty}f^*(x) = 0$, then $x = \alpha - 1$ must be the global maximum point. That is
\begin{equation}\label{eq:rat-a}
 a = \sqrt{f^*(\alpha - 1)} = \sqrt{(\alpha - 1)^{\alpha - 1}e^{-(\alpha - 1)}} = \left(\frac{\alpha - 1}{e}\right)^{(\alpha - 1)/2}.
\end{equation}

We now wish to find $b_+$.

$$
\begin{split}
  0 &= \frac{d}{dx} x^2f^*(x) \\
  &= \frac{d}{dx} x^{\alpha + 1}e^{-x} \\
  &= e^{-x}x^{\alpha}\left((\alpha + 1) - x\right) \\
  \Rightarrow\quad x &= \alpha + 1,\quad \text{where}\quad\alpha > 1.
\end{split}
$$
Using the same reasoning as for $a$, we have that $x = \alpha + 1$ is a global maximum point for $x^2f^*(x)$. Then
\begin{equation}\label{eq:rat-b_plus}
  b_+ = \sqrt{(\alpha + 1)^2 f^*(\alpha + 1)} = \sqrt{(\alpha + 1)^{\alpha + 1}e^{-(\alpha + 1)}} = \left(\frac{\alpha + 1}{e}\right)^{(\alpha + 1)/2}.
\end{equation}

Finally, we have that
\begin{equation}\label{eq:rat-b_minus}
  b_- = -\sqrt{\sup_{x\leq 0}(x^2\cdot 0)} = 0.
\end{equation}

### (b)

To avoid producing `NaNs`, we will implement the ratio-of-uniform method on a log scale.
We get the following log-transformations.
$$
\begin{split}
  X_1\sim \Uniform(0, a)&\Rightarrow \log X_1 = \log a + \log U_1,\quad U_1\sim\Uniform(0,1); \\
  X_2\sim \Uniform(b_-=0, b_+ = b) &\Rightarrow \log X_2 = \log b + \log U_2,\quad U_2\sim\Uniform(0,1); \\
  y = \frac{x_2}{x_1} &\Rightarrow y = \exp\{(\log x_2) - (\log x_1)\}; \\
  0\leq x_1\leq \sqrt{f^*(y)} &\Rightarrow \log x_1 \leq \frac{1}{2}\log f^*(y); \\
  f^*(y) = \begin{cases}y^{\alpha - 1}e^{-y},&y>0, \\ 0, &\text{otherwise},\end{cases} &\Rightarrow \log f^*(y) = \begin{cases}(\alpha - 1)\log y - y,&y>0, \\ -\infty, &\text{otherwise.}\end{cases}
\end{split}
$$

```{r gamma ratio-of-uniform method, fig.cap = "\\label{fig:gamma-trials}Number of trials before accepting $N = 1000$ simulations for various shape parameters ($\\alpha$) using the ratio-of-uniform method."}
set.seed(434)

lgamma_core <- function(x, alpha = 2) {
  ifelse(
    test = x <= 0,
    yes  = -Inf,
    no   = (alpha - 1)*log(x) - x
  )
}

sample_from_gamma_rou <- function(n, alpha = 2, include_trials = FALSE) {
  log_a <- ((alpha - 1) / 2) * (log(alpha - 1) - 1)
  log_b <- ((alpha + 1) / 2) * (log(alpha + 1) - 1)
  trials <- 0
  y <- vector(mode = "numeric", length = n)
  for (i in 1:n) {
    repeat {
      log_x1 <- log_a + log(runif(1))
      log_x2 <- log_b + log(runif(1))
      y[i] <- exp(log_x2 - log_x1)
      log_f <- lgamma_core(y[i], alpha = alpha)
      if (log_x1 <= 0.5 * log_f) {
        break
      } else {
        trials <- trials + 1
      }
    }
  }
  if (include_trials) {
    return(list(x = y, trials = trials))
  }
  return(y)
}

# generate 1000 samples for each alpha and record number of trials
n <- 1000
m <- 50
alpha <- seq(2, 2000, length.out = m)
trials <- vector(mode = "integer", length = m)

for (i in 1:m) {
  trials[i] <- sample_from_gamma_rou(n, alpha[i], include_trials = TRUE)$trials
}

# plot trials wrt. alpha
ggplot(mapping = aes(x = alpha, y = trials)) +
  geom_point()
```

Figure \ref{fig:gamma-trials} strongly suggests that the acceptance probability decreases with increasing $\alpha$. That is, the ratio of the area of the square $a\cdot(b_+ - b_-) = ab$ and the region $C_f$ is increasing when $\alpha$ increases.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------


# Problem C: Monte Carlo integration and variance reduction

## 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem D: Rejection sampling and importance sampling

## 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------