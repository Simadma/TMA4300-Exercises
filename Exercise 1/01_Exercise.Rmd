---
title: "Exercise 1"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE,
                      strip.white = TRUE, prompt = FALSE, cache = TRUE,
                      size = "scriptsize", fig.width = 6, fig.height = 4)
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}
\newcommand{\bfy}{\mathbf{y}}


```{r libraries, eval = TRUE, echo = FALSE}
library(ggplot2)  # Create Elegant Data Visualizations Using the Grammar of Graphics
#library(ISLR)  # Data from the book ISLR
#library(boot)  # Bootstrap Functions
#library(MASS)  # Support Functions and Datasets for Venables and Ripley's MASS
#library(FactoMineR)  # Multivariate Exploratory Data Analysis and Data Mining
#library(factoextra)  # Extract and Visualize the Results of Multivariate Data Analyses
#library(ggfortify)  # Data Visualization Tools for Statistical Analysis Results
#library(glmnet)  # Lasso and Elastic-Net Regularized Generalized Linear Models
#library(tree)  # Classification and Regression Trees
#library(randomForest)  # Breiman and Cutler's Random Forests for Classif. and Regress.
#library(gbm)  # Generalized Boosted Regression Models
#library(keras)  # R Interface to 'Keras'
#library(pls)  # Partial Least Squares and Principal Component Regression
#library(gam)  # Generalized Additive Models
#library(gridExtra)  # Mescellaneous Functions for "Grid" Graphics
#library(GGally)  # Extension to 'ggplot2'
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## 1.



[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)

### (c)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5

[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem B: The gamma distribution

## 1.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------


# Problem C: Monte Carlo integration and variance reduction

## 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem D: Rejection sampling and importance sampling

## Subproblem 1.
We consider a vector of multinomially distributed counts
$$
  \bfy =
  \begin{bmatrix}
    y_1 \\ y_2 \\ y_3 \\ y_4
  \end{bmatrix}
  \quad \text{with probabilities} \quad
  \mathbf{p} =
  \begin{bmatrix}
    \frac 1 2 + \frac \theta 4 \\ \frac{1-\theta}{4} \\ \frac{1-\theta}{4} \\ \frac \theta 4
  \end{bmatrix},
$$
and the observed data is $\bfy = \begin{bmatrix} 125 & 18 & 20 & 34 \end{bmatrix}^\top$. The multinomial mass function is given as
$$
  f(\bfy \mid \theta) \propto (2+\theta)^{y_1} (1-\theta)^{y_2+y_3} \theta^{y_3},
$$
and assuming a prior that is $\Uniform(0, 1)$ the posterior will be
$$
  f(\theta \mid \bfy) \propto f^*(\theta) := (2+\theta)^{y_1} (1-\theta)^{y_2+y_3} \theta^{y_3},
$$
for $\theta \in (0, 1)$. We wish to sample from this using a $\Uniform(0, 1)$ proposal density, that is, $g(\theta \mid \bfy) = 1$, for $\theta \in (0, 1)$. Because we do not know the normalizing constant, we may use weighted resampling, which is an approximate algorithm. We do this by first generating $\Theta_1, \dots, \Theta_n \sim g(\theta) \sim \Uniform(0, 1)$ and then calculate the weights
$$
  w(\Theta_i) = \frac{f(\Theta_i) / g(\Theta_i)}{\sum_{j=1}^n f(\Theta_j) / g(\Theta_j)} = \frac{f(\Theta_i)}{\sum_{j=1}^n f(\Theta_j)},
$$
because $g(\theta) = 1$ for all $\theta \in (0, 1)$. We then generate a second sample of size $m$ from the discrete distribution $\{ \Theta_1, \dots, \Theta_n \}$ with probabilities $w(\Theta_1), \dots, w(\Theta_n)$. This is done in the code block below, where we choose $m$ such that $n/m = 20$.

```{r generate using weighted resampling}

posterior_f_star <- function(theta, y) {
  return((2 + theta)^(y[1]) * (1 - theta)^(y[2] + y[3]) * theta^(y[4]))
}

weighted_resampling_f <- function(n, y) {
  Theta <- runif(n)   # Generate n Uniform(0, 1) variables
  f_star <- posterior_f_star(Theta, y)    # Calculating the vector f_star
  W <- f_star / sum(f_star)   # Calculating the weights
  m <- n / 20   # Using m such that n / m = 20
  X <- sample(Theta, size = m, prob = W)  # Sample m values from Theta with probability W
  return(X)
}
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.
Drawing $\Theta_1, \dots, \Theta_M \sim f(\theta \mid \bfy)$, the Monte Carlo estimate of $\mu = \E(\theta \mid \bfy)$ is
$$
  \hat{\mu} = \frac{1}{M} \sum_{i=1}^M \Theta_i.
$$
We do this for $M = 10000$ in the code block below. Figure \ref{fig:Monte_Carlo} shows the result of this. We see the estimation of the posterior mean $\E(\theta \mid \bfy)$ using Monte Carlo integration and numerical integration together with the theoretical posterior density distribution and a generated histogram of the samples. In the figure the posterior density is plotted using a normalizing constant we find by numerical integration in `R` below, giving the normalizing constant `norm_const`.

```{r Monte Carlo, fig.cap = "\\label{fig:Monte_Carlo}Estimation of the posterior mean $\\E(\\theta \\mid \\bfy)$ using Monte Carlo integration and numerical integration. A histogram of the samples is also shown together with the theoretical posterior density distribution."}
set.seed(69)

M <- 10000    # Number of samples from f(theta | y)
n <- 20 * M   # Number of samples needed to give M samples from f(theta | y)
y <- c(125, 18, 20, 34)   # Observed data
Theta_samp <- weighted_resampling_f(n, y)   # M samples from f(theta | y)
mu_est <- mean(Theta_samp)    # = 1/M * sum(Theta_samp)

# Integrating numerically
norm_const <- integrate(function(theta)(posterior_f_star(theta, y)),
                        lower = 0,
                        upper = 1)$value  # Integrate to find normalizing constant
posterior_f <- function(theta, y) {   # Creating the true posterior f
  return(posterior_f_star(theta, y) / norm_const)
}
mu_num <- integrate(function(theta)(theta * posterior_f(theta, y)),
                    lower = 0,
                    upper = 1)$value    # Value of mu by numerical integration

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(Theta_samp),
    mapping = aes(x = Theta_samp, y = ..density..),
    binwidth = 0.01,
    boundary = 0
  ) +
  stat_function(
    fun = posterior_f,
    args = list(y = y),
    aes(col = "Posterior density")
  ) +
  geom_vline(
    aes(xintercept = c(mu_est, mu_num),
        col = c("Estimated posterior mean", "Numerical posterior mean"),
        linetype = c("dashed", "dotted"))
  ) +
  guides(linetype = FALSE) +    # Remove linetype from label
  ggtitle("Estimation of the posterior mean") +
  xlab("theta") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.title = element_blank())
```

In the following code block we find the values of `mu_est` and `mu_num`.
```{r, eval = TRUE}
mu_est
mu_num
```
From this it is clear that the estimated posterior mean is $\hat{\mu} \approx 0.623$ using Monte Carlo integration, and $\mu \approx 0.623$ using numerical integration with `integrate()`. Figure \ref{fig:Monte_Carlo} also shows that these means corresponds well to the real posterior mean.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 4.


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------