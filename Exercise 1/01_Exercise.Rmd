---
title: "Exercise 1"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE,
                      strip.white = TRUE, prompt = FALSE, cache = TRUE,
                      size = "scriptsize", fig.width = 6, fig.height = 4)
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Exp}{\operatorname{Exponential}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\Betadist}{\operatorname{Beta}}
\newcommand{\Normal}{\operatorname{Normal}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\e}{\mathrm{e}}


```{r libraries, eval = TRUE, echo = FALSE}
library(ggplot2)  # Create Elegant Data Visualizations Using the Grammar of Graphics
#library(ISLR)  # Data from the book ISLR
#library(boot)  # Bootstrap Functions
#library(MASS)  # Support Functions and Datasets for Venables and Ripley's MASS
#library(FactoMineR)  # Multivariate Exploratory Data Analysis and Data Mining
#library(factoextra)  # Extract and Visualize the Results of Multivariate Data Analyses
#library(ggfortify)  # Data Visualization Tools for Statistical Analysis Results
#library(glmnet)  # Lasso and Elastic-Net Regularized Generalized Linear Models
#library(tree)  # Classification and Regression Trees
#library(randomForest)  # Breiman and Cutler's Random Forests for Classif. and Regress.
#library(gbm)  # Generalized Boosted Regression Models
#library(keras)  # R Interface to 'Keras'
#library(pls)  # Partial Least Squares and Principal Component Regression
#library(gam)  # Generalized Additive Models
#library(gridExtra)  # Mescellaneous Functions for "Grid" Graphics
#library(GGally)  # Extension to 'ggplot2'
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## Subproblem 1.
Let $X \sim \Exp(\lambda)$, with the cumulative density function
$$
  F_X(x) = 1 - e^{-\lambda x},\quad x \geq 0.
$$
Then the random variable $Y := F_X(X)$ has a $\Uniform(0, 1)$ distribution. The probability integral transform becomes

$$
  Y = 1 - e^{-\lambda X} \quad \Leftrightarrow \quad X = -\frac{1}{\lambda}\ln(1 - Y).
$$

It is clear that if $U \sim \Uniform(0, 1)$, then $1 - U \sim \Uniform(0, 1)$, and therefore we may as well say that

\begin{equation}\label{eq:exp_transf}
  X = -\frac{1}{\lambda}\ln(Y).
\end{equation}

Thus, we sample $Y$ from `runif()` and transform it using Equation \eqref{eq:exp_transf}, to sample from the exponential distribution. Figure \ref{fig:exp_hist} shows one million samples drawn from the `generate_from_exp()` function defined in the code chunk below. It also shows the theoretical PDF of the exponential distribution with rate parameter $\lambda = 2$.

```{r generate from exp, fig.cap = "\\label{fig:exp_hist}Normalized histogram of one million samples drawn from the exponential distribution, together with the theoretical PDF, with $\\lambda = 2$."}
set.seed(69)

generate_from_exp <- function(n, rate) {
  Y <- runif(n)   # Generate n Uniform(0, 1) variables
  X <- -(1 / rate) * log(Y)   # Transformation
  return(X)
}

# sample
n <- 1000000  # One million samples
lambda <- 2
exp_samp <- generate_from_exp(n, lambda)

# plot
ggplot() +
  geom_histogram(
    data = as.data.frame(exp_samp),
    mapping = aes(x = exp_samp, y = ..density..),
    binwidth = 0.05,
    boundary = 0
  ) +
  stat_function(
    fun = dexp,
    args = list(rate = lambda),
    aes(col = "Theoretical density")
  ) +
  ylim(0, lambda) +
  xlim(0, 2) +
  ggtitle("Simulating from an exponential distribution") +
  xlab("x") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Theoretically, the mean and variance of $X \sim \Exp(\lambda)$ is $\E(X) = \lambda^{-1}$ and $\Var(X) = \lambda^{-2}$. So for $\lambda = 2$ we would expect $\E(X) = 1/2$ and $\Var(X) = 1/4$. For the simulation we get the mean and variance as calculated in the code block below, showing what we would expect.
```{r, eval = TRUE}
mean(exp_samp)
var(exp_samp)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.

### Subsubproblem (a)

We are considering the probability density function
\begin{equation} \label{eq:gx}
  g(x) =
  \begin{cases}
    c x^{\alpha-1} & \text{if } 0 < x < 1,
    \\
    c \mathrm{e}^{-x} & \text{if } x \geq 1,
    \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}
where $c$ is a normalizing constant and $\alpha \in (0, 1)$. If $x \leq 0$ the cumulative distribution function is zero. In the interval $0 < x < 1$ it becomes
$$
  G(x) = \int_{-\infty}^x g(\xi) \,\mathrm{d}\xi = \int_0^x c \xi^{\alpha - 1} \,\mathrm{d}\xi = \frac{c}{\alpha} [\xi^\alpha]_0^x = \frac{c}{\alpha} x^\alpha,
$$
and finally for $x \geq 1$ we have
$$
  G(x) = \int_{-\infty}^x g(\xi) \,\mathrm{d}\xi = \int_0^1 c \xi^{\alpha - 1} \,\mathrm{d}\xi + \int_1^x c \mathrm{e}^{-\xi} \,\mathrm{d}\xi = \left[ \frac{c}{\alpha} \xi^{\alpha} \right]_0^1 - \left[ c \mathrm{e}^{-\xi} \right]_1^x = c \left( \frac{1}{\alpha} - \mathrm{e}^{-x} + \frac{1}{\mathrm{e}} \right),
$$
for $\alpha \in (0, 1)$. That is, the cumulative density function is
$$
  G(x) =
  \begin{cases}
    0 & \text{if } x \leq 0,
    \\
    \frac{c}{\alpha} x^\alpha & \text{if } 0 < x < 1,
    \\
    c \left( \frac{1}{\alpha} - \mathrm{e}^{-x} + \frac{1}{\mathrm{e}} \right) & \text{if } x \geq 1.
  \end{cases}
$$
In this case it is trivial to find $c$. We solve
$$
  1 = \int_\mathbb{R} g(x) \,\mathrm{d}x = \int_0^1 c x^{\alpha - 1} \,\mathrm{d}x + \int_1^\infty c \mathrm{e}^{-x} \,\mathrm{d}x = \frac{c}{\alpha} + \frac{c}{\mathrm{e}},
$$
which gives that
$$
  c = \frac{\alpha \mathrm{e}}{\alpha + \mathrm{e}}.
$$
Writing the cumulative density function using this as $c$ we obtain
$$
  G(x) =
  \begin{cases}
    0 & \text{if } x \leq 0,
    \\
    \frac{\mathrm{e}}{\alpha + \mathrm{e}} x^\alpha & \text{if } 0 < x < 1,
    \\
    1 - \frac{\alpha}{\alpha + \mathrm{e}} \mathrm{e}^{1-x} & \text{if } x \geq 1,
  \end{cases}
$$
for $\alpha \in (0, 1)$.

We may then find the inverse cumulative function. For $x \leq 0$ this is just zero, and for $0 < x < 1$, that is $0 < G(x) < \frac{\mathrm{e}}{\alpha + \mathrm{e}}$, we solve $x = \frac{\mathrm{e}}{\alpha + \mathrm{e}} y^\alpha$ for $y$ giving $G^{-1}(x) = \left( \frac{\alpha + \mathrm{e}}{\mathrm{e}} x \right)^{1/\alpha}$. Similarly for $x \geq 1$, that is $G(x) \geq 1 - \frac{\alpha}{\alpha + \mathrm{e}} = \frac{\mathrm{e}}{\alpha + \mathrm{e}}$, we solve $x = 1 - \frac{\alpha}{\alpha + \mathrm{e}} \mathrm{e}^{1-y}$ for $y$, such that
$$
  G^{-1}(x) =
  \begin{cases}
    \left( \frac{\alpha + \mathrm{e}}{\mathrm{e}} x \right)^{1/\alpha} & \text{if } 0 \leq x < \frac{\mathrm{e}}{\alpha + \mathrm{e}},
    \\
    \ln\left[ \frac{\alpha \mathrm{e}}{(1-x) (\alpha + \mathrm{e})} \right] & \text{if } \frac{\mathrm{e}}{\alpha + \mathrm{e}} \leq x \leq 1,
  \end{cases}
$$
for $\alpha \in (0, 1)$.


### Subsubproblem (b)
Using what we found in (a) we may use the inversion method to sample from $g(x)$ given in Equation \eqref{eq:gx}, as shown in the code block beneath. Figure \ref{fig:g_hist} shows one million samples drawn from `generate_from_gx()` and also the theoretical PDF.

```{r generate from gx, fig.cap = "\\label{fig:g_hist}Normalized histogram of one million samples drawn from $g(x)$ given in Equation \\eqref{eq:gx}, together with the theoretical PDF, with $\\alpha = 0.75$."}
set.seed(69)

generate_from_gx <- function(n, alpha) {
  U <- runif(n)   # Generate n Uniform(0, 1) variables
  bound <- exp(1) / (alpha + exp(1))   # Boundary where G^(-1) changes
  left <- U < bound   # The left of the boundary
  U[left] <- (U[left] / bound)^(1 / alpha)   # Left CDF
  U[!left] <- 1 + log(alpha) - log(1 - U[!left]) - log(alpha + exp(1))   # Right CDF
  return(U)
}

# Sample
n <- 1000000  # One million samples
alpha <- 0.75
gx_samp <- generate_from_gx(n, alpha)   # Generating n samples from g(x)

# The theoretically correct PDF
theo_gx <- function(x, alpha) {
  const <- alpha * exp(1) / (alpha + exp(1))    # Normalizing constant
  func <- rep(0, length(x))   # Vector of zeros of same length as x
  left <- x > 0 & x < 1   # The PDF has one value for 0 < x < 1
  right <- x >= 1         # ... and one value for x >= 1
  func[left] <- const * x[left]^(alpha - 1)   # The value to the left
  func[right] <- const * exp(-x[right])       # The value to the right
  return(func)
}

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(gx_samp),
    mapping = aes(x = gx_samp, y = ..density..),
    binwidth = 0.05,
    boundary = 0
  ) +
  stat_function(
    fun = theo_gx,
    args = list(alpha = alpha),
    aes(col = "Theoretical density")
  ) +
  xlim(0, 5) +
  ggtitle("Simulating from g(x) given in Equation (2)") +
  xlab("x") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Assuming $X \sim g(x)$ we may find the expectation to be
$$
  \E(X) = \int_{\mathbb{R}} x g(x) \,\mathrm{d}x = \int_0^1 c x^\alpha \,\mathrm{d}x + \int_1^\infty c x \mathrm{e}^{-x} \,\mathrm{d}x = \frac{\alpha \e}{(\alpha+1)(\alpha+\e)} + \frac{2 \alpha}{\alpha + \e} \approx 0.768,
$$
when $\alpha = 0.75$. This corresponds approximately to the sample mean shown in the following code block.
```{r, eval = TRUE}
mean(gx_samp)
```
Similarly we may find the theoretical variance to be
$$
  \Var(X) = \E(X^2) - \E(X)^2 \approx 0.705,
$$
also for $\alpha = 0.75$, corresponding approximately to the sample variance given in the code block below.
```{r, eval = TRUE}
var(gx_samp)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.

### Subsubproblem (a)
We consider the probability density function
$$
  f(x) = \frac{c \e^{\alpha x}}{(1 + \e^{\alpha x})^2},
$$
for $-\infty < x < \infty$ and $\alpha > 0$. To find the normalizing constant $c$ we make sure that the integral over $\mathbb{R}$ of $f(x)$ is one. That is
$$
  1 = \int_\mathbb{R} f(x) \,\mathrm{d}x = c\int_\mathbb{R} \frac{ \e^{\alpha x}}{(1 + \e^{\alpha x})^2} \,\mathrm{d}x,
$$
and letting $u = 1 + \e^{\alpha x}$, it follows that
$$
  1 = \frac{c}{\alpha} \int_1^\infty \frac{\mathrm{d}u}{u^2} = \frac{c}{\alpha} \left[ -\frac{1}{u} \right]_1^\infty = \frac{c}{\alpha}.
$$
That is, the normalizing constant is $c = \alpha$, for $\alpha > 0$. We may then write the probability density function as
\begin{equation} \label{eq:fx}
  f(x) = \frac{\alpha \e^{\alpha x}}{(1 + \e^{\alpha x})^2},
\end{equation}
for $-\infty < x < \infty$ and $\alpha > 0$.


### Subsubproblem (b)
The cumulative distribution function is given as
$$
  F(x) = \int_{-\infty}^x f(\xi) \,\mathrm{d}\xi = \int_{-\infty}^x \frac{\alpha \e^{\alpha \xi}}{(1 + \e^{\alpha \xi})^2} \,\mathrm{d}\xi.
$$
Again letting $u = 1 + \e^{\alpha \xi}$ it follows that
$$
  F(x) = \int_{1}^{1+\e^{\alpha x}} \frac{\alpha \e^{\alpha \xi}}{u^2} \frac{\mathrm{d}u}{\alpha \e^{\alpha \xi}} = \int_{1}^{1+\e^{\alpha x}} \frac{\mathrm{d}u}{u^2} = \left[ \frac{1}{u} \right]_{1+\e^{\alpha x}}^1 = 1 - \frac{1}{1+\e^{\alpha x}} = \frac{\e^{\alpha x}}{1+\e^{\alpha x}},
$$
which holds for $-\infty < x < \infty$ and $\alpha > 0$.

Solving $x = \e^{\alpha y} / (1 + \e^{\alpha y})$ for $y$ then gives us the inverse cumulative distribution function. Some algebra then gives that
$$
  F^{-1}(x) = \frac{1}{\alpha} \ln\left( \frac{x}{1-x} \right) = \frac{1}{\alpha} \left[ \ln\left( x \right) - \ln\left( 1-x \right) \right],
$$
for $0 < x < 1$ and $\alpha > 0$.


### Subsubproblem (c)

Simulating from $f(x)$ given in Equation \eqref{eq:fx} is shown in the code block below, and the result is shown in Figure \ref{fig:f_hist}. The sampling is done by letting $U \sim \Uniform(0, 1)$ and using inversion sampling.

```{r generate from fx, fig.cap = "\\label{fig:f_hist}Normalized histogram of one million samples drawn from $f(x)$ given in Equation \\eqref{eq:fx}, together with the theoretical PDF, with $\\alpha = 100$."}

generate_from_fx <- function(n, alpha) {
  U <- runif(n)   # Generate n Uniform(0, 1) variables
  X <- 1 / alpha * (log(U) - log(1 - U))    # Using the inverse CDF
  return(X)
}

# Sample
n <- 1000000  # One million samples
alpha <- 100  # Letting alpha be 100
fx_samp <- generate_from_fx(n, alpha)   # Generating n samples from f(x)

# The theoretically correct PDF
theo_fx <- function(x, alpha) {
  return(alpha * exp(alpha * x) / (1 + exp(alpha * x))^2)
}

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(fx_samp),
    mapping = aes(x = fx_samp, y = ..density..),
    binwidth = 0.001,
    boundary = 0
  ) +
  stat_function(
    fun = theo_fx,
    args = list(alpha = alpha),
    aes(col = "Theoretical density")
  ) +
  ggtitle("Simulating from f(x) given in Equation (3)") +
  xlab("x") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Letting $X \sim f(x)$ we may find the expected value to be
$$
  \E(X) = \int_\mathbb{R} xf(x) \,\mathrm{d}x = \int_\mathbb{R} \frac{\alpha x \e^{\alpha x}}{(1 + \e^{\alpha x})^2} \,\mathrm{d}x = 0,
$$
because of symmetry. This is confirmed in the following code block where we see that the sample mean is approximately zero, and can also be seen from the figure. This holds for all $\alpha > 0$.
```{r, eval = TRUE}
mean(fx_samp)
```
We may also find the variance of $X$ to be
$$
  \Var(X) = \E(X^2) = \int_\mathbb{R} \frac{\alpha x^2 \e^{\alpha x}}{(1 + \e^{\alpha x})^2} \,\mathrm{d}x \approx 0.000329,
$$
for $\alpha = 100$. This also corresponds to the sample variance as shown in the code block below.
```{r, eval = TRUE}
var(fx_samp)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 4.
We wish to simulate from a $\Normal(0, 1)$ distribution using the Box-Muller algoritm. If $X_1 \sim \Uniform(0, \pi)$ and $X_2 \sim \Exp(1/2)$, then $Y_1 = \sqrt{X_2} \cos(X_1)$ and $Y_2 = \sqrt{X_2} \sin(X_1)$ are standard normal distributed. We use $Z = \sqrt{X_2} \cos(X_1)$ in the following code block. The result of the simulation can be seen in Figure \ref{fig:Box_Muller}, and it also shows the theoretical probability density function.

```{r generate from Box-Muller, fig.cap = "\\label{fig:Box_Muller}One million samples drawn from a standard normal distribution using the Box-Muller algorithm, together with the theoretical PDF."}

std_normal <- function(n) {
  X1 <- pi * runif(n)   # n samples from Uniform(0, pi)
  X2 <- generate_from_exp(n, 1/2)   # n samples from Exponential(1/2)
  Z <- X2^(1/2) * cos(X1)   # Z ~ Normal(0, 1)
  return(Z)
}

# Sample
n <- 1000000  # One million samples
Box_Muller <- std_normal(n)   # Generating n samples from Normal(0, 1)

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(Box_Muller),
    mapping = aes(x = Box_Muller, y = ..density..),
    binwidth = 0.05,
    boundary = 0
  ) +
  stat_function(
    fun = dnorm,
    aes(col = "Theoretical density")
  ) +
  ggtitle("Simulating from standard normal distribution") +
  xlab("z") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

We know that if $Z \sim \Normal(0, 1)$, then $\E(Z) = 0$ and $\Var(Z) = 1$, and this corresponds to the approximate sample mean and variance shown in the code block below.
```{r, eval = TRUE}
mean(Box_Muller)
var(Box_Muller)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 5.
We wish to simulate from a $d$-variate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$. Let $Z \sim \Normal_d(0, I_d)$, where $I_d$ is the identity matrix in $\mathbb{R}^{d \times d}$ and $0$ is the zero-vector in $\mathbb{R}^{d}$. Then
$$
  X = \mu + A Z \sim \Normal_d(\mu, A A^\top),
$$
such that we need to find $A$ such that $\Sigma = A A^\top$, this is done using `chol()` in `R`, and we construct the function in the following code block.

```{r generate from d-variate normal}

d_variate_normal <- function(n, mu, Sigma) {
  d <- length(mu)   # The dimension d is the dimension of mu
  A <- t(chol(Sigma)) # Cholesky decomposition of Sigma. Transpose to get lower triangular
  z <- std_normal(d * n)    # Create vector of d*n independent Normal(0, 1)
  Z <- matrix(z, nrow = d, ncol = n)    # Make z into (d X n) matrix Z
  X <- mu + A %*% Z   # X ~ Normal_d(mu, Sigma) 
  return(X)
}
```
We now test the implementation using one million samples in $\mathbb{R}^3$, with
$$
  \mu =
  \begin{bmatrix}
    1 \\ 7 \\ 2
  \end{bmatrix}
  \quad \text{and} \quad
  \Sigma =
  \begin{bmatrix}
    2 & -1 & 0 
    \\
    -1 & 2 & -1 
    \\
    0 & -1 & 2
  \end{bmatrix}.
$$
We then expect the sample mean and sample covariance matrix to be approximately equal to these. In the following code block we see that this is indeed the case.
```{r, eval = TRUE}
# Sample
n <- 1000000    # One million samples
mu <- c(1, 7, 2)   # Create mu
Sigma <- matrix(c(2, -1, 0, -1, 2, -1, 0, -1, 2), nrow = 3)    # Create Sigma
normal <- d_variate_normal(n, mu, Sigma)    # Sample from d-variate Normal(mu, Sigma)

# Test
rowMeans(normal)  # Finding the mean of the rows of normal, sample mean
cov(t(normal))    # Transpose because we want with respect to the rows
```


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem B: The gamma distribution

## 1.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------


# Problem C: Monte Carlo integration and variance reduction

## 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem D: Rejection sampling and importance sampling

## Subproblem 1.
We consider a vector of multinomially distributed counts $\bfy = \begin{bmatrix} y_1 & y_2 & y_3 & y_4 \end{bmatrix}^\top$ and the observed data is $\bfy = \begin{bmatrix} 125 & 18 & 20 & 34 \end{bmatrix}^\top$. The multinomial mass function is given as
$$
  f(\bfy \mid \theta) \propto (2+\theta)^{y_1} (1-\theta)^{y_2+y_3} \theta^{y_3},
$$
and assuming a prior that is $\Uniform(0, 1)$ the posterior will be
$$
  f(\theta \mid \bfy) \propto f^*(\theta) := (2+\theta)^{y_1} (1-\theta)^{y_2+y_3} \theta^{y_3},
$$
for $\theta \in (0, 1)$. We wish to sample from this using a $\Uniform(0, 1)$ proposal density, that is, $g(\theta \mid \bfy) = 1$, for $\theta \in (0, 1)$. To do a rejection sampling (not weighted rejection sampling), we need to know the normalizing constant of $f(\theta \mid \bfy)$. That is, the constant $K$ such that $f(\theta \mid \bfy) = K f^*(\theta \mid \bfy)$. This can be found as
$$
  \frac 1 K = \int_\mathbb{R} f^*(\theta \mid \bfy) \, \mathrm{d} \theta = \int_0^1 f^*(\theta \mid \bfy) \, \mathrm{d} \theta \approx 2.3577 \cdot 10^{28},
$$
and we find it using the `integrate()`-function in `R` below. To use the rejection sampling we also need that
$$
  \frac{f(\theta \mid \bfy)}{g(\theta \mid \bfy)} = f(\theta \mid \bfy) \leq k,
$$
and a value for $k$ is found in the code block below. This is done by setting $k = \max_\theta f^*(\theta \mid \bfy)$, and for the observed $\bfy$, we have that
$$
  \frac{\mathrm{d} f^*(\theta \mid \bfy)}{\mathrm{d} \theta} = (\theta-1)^{37} \theta^{33} (\theta + 2)^{124} (197 \theta^2 - 15 \theta - 68) = 0.
$$
Because $\theta \in (0, 1)$, the only factor we need to consider is $197 \theta^2 - 15 \theta - 68 = 0$, giving
$$
  \theta_\mathrm{max} = \frac{15 + \sqrt{53809}}{394},
$$
such that $k = f^*(\theta_\mathrm{max} \mid \bfy)$. This however is found numerically with the `optimize()`-function in `R` below. We then simulate $\Theta \sim \Uniform(0, 1)$ and $U \sim \Uniform(0, 1)$ and calculate $\alpha = f(\theta \mid \bfy) / k$. Then, if $U \leq \alpha$, $\Theta$ is returned, and if not, the procedure is run again. We then sample from the posterior distribution in the code block below.

```{r generate using rejection sampling}

y <- c(125, 18, 20, 34)   # Observed data

# Define the un-normalized posterior distribution f*(theta | y)
posterior_star <- function(theta, y) {
  return((2 + theta)^(y[1]) * (1 - theta)^(y[2] + y[3]) * theta^(y[4]))
}

# Find the normalizing constant 1 / K
norm_const <- integrate(function(theta)(posterior_star(theta, y)),
                        lower = 0,
                        upper = 1)$value

# Defining the normalized posterior distribution f(theta | y)
posterior <- function(theta, y) {
  return(posterior_star(theta, y) / norm_const)
}

# Finding the maximum 
posterior_star_max <- optimize(function(theta)(posterior_star(theta, y)),
                               interval = c(0, 1),
                               maximum = TRUE)$objective

# k such that f(theta | y) <= k
k <- posterior_star_max / norm_const

# Rejection sampling algorithm
rejection_sampling <- function(M, y) {
  count <- 0      # Count how many times the algorithm runs
  accept <- c()   # List of the accepted samples
  
  while(length(accept) < M) {   # Running until we have M accepted samples
    U <- runif(1)   # One Uniform(0, 1) sample
    Theta <- runif(1)   # One Uniform(0, 1) sample
    alpha <- posterior(Theta, y) / k
    if(U <= alpha) {
      accept <- rbind(accept, Theta)    # Appending to the list of accepted samples
    }
    count <- count + 1    # while loop has run one more time
  }
  return(list("accept" = accept, "co" = count))
}
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.
Drawing $\Theta_1, \dots, \Theta_M \sim f(\theta \mid \bfy)$, the Monte Carlo estimate of $\mu = \E(\theta \mid \bfy)$ is
$$
  \hat{\mu} = \frac{1}{M} \sum_{i=1}^M \Theta_i.
$$
We do this for $M = 10000$ in the code block below. Figure \ref{fig:Monte_Carlo} shows the result of this. We see the estimation of the posterior mean $\E(\theta \mid \bfy)$ using Monte Carlo integration and numerical integration together with the theoretical posterior density distribution and a generated histogram of the samples. In the figure the posterior density is plotted using a normalizing constant we find by numerical integration in `R`.

```{r Monte Carlo, fig.cap = "\\label{fig:Monte_Carlo}Estimation of the posterior mean $\\E(\\theta \\mid \\bfy)$ using Monte Carlo integration and numerical integration. A histogram of the samples is also shown together with the theoretical posterior density distribution."}

set.seed(69)

M <- 10000    # Number of samples from f(theta | y)

Theta_samp <- rejection_sampling(M, y)   # M samples from f(theta | y)
mu_est <- mean(Theta_samp$accept)    # = 1/M * sum(Theta_samp)

mu_num <- integrate(function(theta)(theta * posterior(theta, y)),
                    lower = 0,
                    upper = 1)$value    # Value of mu by numerical integration

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(Theta_samp$accept),
    mapping = aes(x = Theta_samp$accept, y = ..density..),
    binwidth = 0.01,
    boundary = 0
  ) +
  stat_function(
    fun = posterior,
    args = list(y = y),
    aes(col = "Posterior density")
  ) +
  geom_vline(
    aes(xintercept = c(mu_est, mu_num),
        col = c("Estimated posterior mean", "Numerical posterior mean"),
        linetype = c("dashed", "dotted"))
  ) +
  guides(linetype = FALSE) +    # Remove linetype from label
  ggtitle("Estimation of the posterior mean") +
  xlab("theta") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.title = element_blank())
```

In the following code block we find the values of `mu_est` and `mu_num`.
```{r, eval = TRUE}
mu_est
mu_num
```
From this it is clear that the estimated posterior mean is $\hat{\mu} \approx 0.624$ using Monte Carlo integration, and $\mu \approx 0.623$ using numerical integration with `integrate()`. Figure \ref{fig:Monte_Carlo} also shows that these means corresponds well to the real posterior mean.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.
We are now interested in the number of random numbers the sampling algorithm needs to obtain one sample from $f(\theta \mid \bfy)$. The expected number of trials up to the first sample from $f(\theta \mid \bfy)$ is $c$ given by the condition
$$
  \frac{f(\theta \mid \bfy)}{g(\theta \mid \bfy)} = f(\theta \mid \bfy) \leq k.
$$
We may then choose
$$
  k \geq \max_{\theta \in [0, 1]} f(\theta \mid \bfy),
$$
and we chose the equality earlier. This was found earlier and stored in the variable `k`, which we can see the value of in the code block below.
```{r, eval = TRUE}
k
```
Using the sampler, the expected number of random numbers that has to be generated in order to obtain one sample of $f(\theta \mid \bfy)$ is the amount of times the ´while´ loop runs divided by the length amount of samples of $f(\theta \mid \bfy)$. This value is given in the following code block.
```{r using the sampling}
Theta_samp$co / M
```
These corresponds well, and we see that we need to generate approximately $7.8$ random numbers in order to obtain one sample of $f(\theta \mid \bfy)$.



## Subsection 4.

We now assume a $\Betadist(1, 5)$ prior
$$
  \tilde{f}(\theta) = \frac{1}{\mathrm{B}(1, 5)} (1 - \theta)^4 \propto (1 - \theta)^4,
$$
where $\mathrm{B}(\cdot, \cdot)$ is the beta function. This gives the posterior
$$
  \tilde{f}(\theta \mid \bfy) \propto f(\bfy \mid \theta) \tilde{f}(\theta) \propto \tilde{f}^*(\theta \mid \bfy) := (2 + \theta)^{y_1} (1 - \theta)^{y_2 + y_3 + 4} \theta^{y_4}.
$$
We wish to estimate $\mu$ by importance sampling, and to avoid needing to know the normalizing constant, we use the self-normalizing importance sampling estimator
$$
  \tilde{\mu}_\mathrm{IS} = \frac{\sum_{i=1}^n \Theta_i w(\Theta_i)}{\sum_{i=1}^n w(\Theta_i)},
$$
where
$$
  w(\Theta_i) = \frac{\tilde{f}^*(\Theta_i \mid \bfy)}{f^*(\Theta_i)} = (1 - \Theta_i)^4,
$$
and $\Theta_1, \dots, \Theta_n \sim f(\theta \mid \bfy)$. In the limit $n \to \infty$, the estimator $\tilde{\mu}_\mathrm{IS}$ is unbiased.

```{r importance sampling}
set.seed(69)

posterior_tilde_star <- function(theta, y) {
  return((2 + theta)^(y[1]) * (1 - theta)^(y[2] + y[3] + 4) * theta^(y[4]))
}

importance_sampling <- function(n, y) {
  Theta <- rejection_sampling(n, y)$accept    # Sample Theta from f(theta | y)
  w <- (1 - Theta)^4    # Calculate the weights w
  importance_mean <- sum(Theta * w) / sum(w)    # Self-normalizing importance sampling
  return(importance_mean)
}

# Test
n <- 10000
y <- c(125, 18, 20, 34)   # Observed data

mu_is <- importance_sampling(n, y)    # Estimated using importance sampling
mu_is

# Finding mu using numerical integration
const <- integrate(function(theta)(posterior_tilde_star(theta, y)),
                   lower = 0,
                   upper = 1)$value
true_mu <- integrate(function(theta)(theta * posterior_tilde_star(theta, y) / const),
                     lower = 0,
                     upper = 1)$value
true_mu

```

We then see that the estimated $\tilde{\mu}_\mathrm{IS} \approx 0.5948$, while the posterior mean using numerical integration is $\mu \approx 0.5959$, both of which corresponds well. We notice that $\tilde{\mu}_\mathrm{IS} < \hat{\mu}$, as found in Subproblem 2. This can be explained by looking at the priors. The previous prior $\Betadist(1, 1)$ and the new prior $\Betadist(1, 5)$ are shown in Figure \ref{fig:priors}. From this it is clear that the $\Betadist(1, 5)$ prior favors lower values for $\theta$ compared to the uniform prior, which do not favor any particular $\theta$. It is therefore expected that $\tilde{\mu}_\mathrm{IS} < \hat{\mu}$, which is shown to be true.

```{r priors, fig.cap = "\\label{fig:priors}The two priors $\\Betadist(1, 1)$ and $\\Betadist(1, 5)$."}
ggplot() +
  stat_function(
    fun = dbeta,
    args = list(shape1 = 1, shape2 = 1),
    aes(col = "Beta(1, 1)")
  ) +
  stat_function(
    fun = dbeta,
    args = list(shape1 = 1, shape2 = 5),
    aes(col = "Beta(1, 5)")
  ) +
  ggtitle("The different priors") +
  xlab("theta") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.title = element_blank())
```



[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------