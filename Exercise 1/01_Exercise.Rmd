---
title: "Exercise 1"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE,
                      strip.white = TRUE, prompt = FALSE, cache = TRUE,
                      size = "scriptsize", fig.width = 6, fig.height = 4)
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}


```{r libraries, eval = TRUE, echo = FALSE}
library(ggplot2)  # Create Elegant Data Visualizations Using the Grammar of Graphics
#library(ISLR)  # Data from the book ISLR
#library(boot)  # Bootstrap Functions
#library(MASS)  # Support Functions and Datasets for Venables and Ripley's MASS
#library(FactoMineR)  # Multivariate Exploratory Data Analysis and Data Mining
#library(factoextra)  # Extract and Visualize the Results of Multivariate Data Analyses
#library(ggfortify)  # Data Visualization Tools for Statistical Analysis Results
#library(glmnet)  # Lasso and Elastic-Net Regularized Generalized Linear Models
#library(tree)  # Classification and Regression Trees
#library(randomForest)  # Breiman and Cutler's Random Forests for Classif. and Regress.
#library(gbm)  # Generalized Boosted Regression Models
#library(keras)  # R Interface to 'Keras'
#library(pls)  # Partial Least Squares and Principal Component Regression
#library(gam)  # Generalized Additive Models
#library(gridExtra)  # Mescellaneous Functions for "Grid" Graphics
#library(GGally)  # Extension to 'ggplot2'
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## 1.
Let $X \sim \Exp(\lambda)$, with the cdf
$$
  F_X(x) = 1 - e^{-\lambda x},\quad x \geq 0.
$$
Then the random variable $Y := F_X(X)$ has a $\Uniform(0, 1)$ distribution. The probability integral transform becomes

\begin{equation}\label{eq:exp_transf}
  Y = 1 - e^{-\lambda X} \Leftrightarrow X = -\frac{1}{\lambda}\log(1 - Y).
\end{equation}

Thus, we sample $Y$ from `runif()` and transform it using \eqref{eq:exp_transf}, to sample from the exponential distribution. Figure \ref{fig:exp_hist} shows one million samples drawn from the `generate_from_exp()` function defined in the code chunk below.

```{r generate from exp, fig.cap = "\\label{fig:exp_hist}Normalized histogram of one million samples drawn from the exponential distribution, together with the theoretical pdf, with $\\lambda = 4.32$."}
set.seed(123)

generate_from_exp <- function(n, rate = 1) {
  Y <- runif(n)
  X <- -(1 / rate) * log(1 - Y)
  X
}

# sample
n <- 1000000  # One million samples
lambda <- 4.32
x <- generate_from_exp(n, rate = lambda)

# plot
hist(x,
  breaks      = 80,
  probability = TRUE,
  xlim        = c(0, 2)
)
curve(dexp(x, rate = lambda),
  add = TRUE,
  lwd = 2,
  col = "red"
)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)

### (c)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5

[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem B: The gamma distribution

## 1.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 5.

### (a)

### (b)


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------


# Problem C: Monte Carlo integration and variance reduction

## 1.

Let $X\sim \operatorname{N}(0,1)$, and $\theta = \Pr(X > 4) \approx `r pnorm(4, lower.tail = FALSE)`$. Let also $h(x) = I(x > 4)$, where $I(\cdot)$ is the indicator function. Then
$$
\E[h(X)] = \int_{-\infty}^\infty h(x)f_X(x)\,dx = \int_{-\infty}^\infty I(x > 4) f_X(x)\,dx = \Pr(X>4) = \theta.
$$

Let $X_1,\ldots X_n\sim\operatorname{N}(0,1)$ be a sample. Then the simple Monte Carlo estimator of $\theta$ is
$$
  \hat\theta_{\mathrm{MC}} = \frac{1}{n}\sum_{i=1}^n h(X_i),
$$
with expectation
$$
  \E\left[\hat\theta_\mathrm{MC}\right] = \frac{1}{n}\sum_{i=1}^n\E\left[h(X_i)\right] = \frac{1}{n}\sum_{i=1}^n\theta = \theta,
$$

and sampling variance
$$
  \widehat{\Var}\left[\hat\theta_\mathrm{MC}\right] = \frac{1}{n^2}\sum_{i=1}^n\widehat{\Var}\left[h(X_i)\right]= \frac{1}{n}\widehat{\Var}[h(X)]=\frac{1}{n(n-1)}\sum_{i=1}^n\left(h(X_i) - \hat\theta_\mathrm{MC}\right)^2.
$$

Then the statistic
$$
  T = \frac{\hat\theta_\mathrm{MC} - \theta}{\sqrt{\widehat{\Var}\left[\hat\theta_\mathrm{MC}\right]}}\sim\mathrm t_{n - 1},
$$
and $t_{\alpha/2,\,n-1} = F_T^{-1}(1 - \alpha/2)$, where $F_T^{-1}(\cdot)$ is the quantile function of the $\mathrm{t}_{n-1}$ distribution.

```{r Monte Carlo integration}
#remove this------------------------------------------------------------------------------
generate_from_exp <- function(n, rate = 1) {
  Y <- runif(n)
  X <- -(1 / rate) * log(Y)
  return(X)
}
std_normal <- function(n) {
  X1 <- pi * runif(n)   # n samples from Uniform(0, pi)
  X2 <- generate_from_exp(n, 1/2)   # n samples from Exponential(1/2)
  Z <- X2^(1/2) * cos(X1)   # Z ~ Normal(0, 1)
  return(Z)
}
#remove this------------------------------------------------------------------------------

set.seed(321)
theta <- pnorm(4, lower.tail = FALSE)
n <- 100000
x <- std_normal(n)
h <- function(x) {
  1 * (x > 4)
}

theta_MC <- sum(h(x)) / n  # Monte Carlo estimate of Pr(X > 4)

sample_var_MC <- sum((h(x) - theta_MC)^2) / (n - 1)  # Sampling variance

t <- qt(0.05/2, df = n - 1, lower.tail = FALSE)  # quantile with 5% significance level
ci_MC <- theta_MC + t * sqrt(sample_var_MC / n) * c(-1, 1)  # Confidence Interval

# Result
list(
  theta_MC      = theta_MC,
  sample_var_MC = sample_var_MC,
  confint       = ci_MC,
  error         = abs(theta_MC - theta)
)
```

[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.
We will sample from the proposal distribution
$$
  g_X(x) = \begin{cases}cx e^{-\frac{1}{2}x^2}, & x > 4 \\ 0, & \text{otherwise}.\end{cases}
$$
but first we must find the normalizing constant $c$.

$$
\begin{split}
  c &= \left(\int_{4}^\infty x e^{-\frac{1}{2}x^2}\,dx\right)^{-1} 
  = \left(\int_{\frac{1}{2}4^2}^\infty e^{-u}\,du\right)^{-1} 
  = \left(e^{-\frac{1}{2}4^2} - 0\right)^{-1} 
  = e^{\frac{1}{2}4^2}, \\
  \Rightarrow g_X(x) &= \begin{cases}x e^{-\frac{1}{2}(x^2 - 4^2)}, & x > 4, \\ 0, & \text{otherwise}.\end{cases}
\end{split}
$$

We can easily sample from the proposal distribution using inversion sampling. The cdf for $x>4$ is found by integrating.
$$
  G_X(x) = \int_4^x y e^{-\frac{1}{2}(y^2 - 4^2)}\,dy = \int_0^{\frac{1}{2}(x^2 - 4^2)}e^{-u}\,du = 1 - e^{-\frac{1}{2}(x^2 - 4^2)},\quad x> 4,
$$
and $G_X(x) = 0$ for $x \leq 4$. Let $U = G_X(X)\sim \Uniform(0, 1)$. Then we solve for $X$.
$$
\begin{split}
  U &= 1 - e^{-\frac{1}{2}(X^2 - 4^2)} \\
  -\frac{1}{2}(X^2 - 4^2) &= \log(1 - U) \\
  X &= \sqrt{4^2 -2\log(1 - U)}, \quad U\sim \Uniform(0, 1).
\end{split}
$$

Let $X_1,\ldots,X_n$ be a sample drawn from the proposal distribution $g_X(x)$. Then the importance sampling estimator of $\theta$ is given by
$$
  \hat\theta_\mathrm{IS} = \frac{1}{n}\sum_{i=1}^n h(X_i)w(X_i),
$$
where $w(x) = f_X(x)/g_X(x)$, with expectation

$$
\begin{split}
\E\left[\hat\theta_\mathrm{IS}\right] &= \frac{1}{n}\sum_{i=1}^n\int_{0}^\infty h(x_i)w(x_i)g_X(x_i)\,dx_i \\
&= \frac{1}{n}\sum_{i=1}^n\int_{0}^\infty h(x_i)f_X(x_i)\,dx_i \\
&= \frac{1}{n}\sum_{i=1}^n\E\left[h(X_i)\mid X_i\sim \operatorname{N}(0, 1)\right] \\
&= \frac{1}{n}\sum_{i=1}^n\theta \\
&= \theta,
\end{split}
$$

and sampling variance

$$
  \widehat{\Var}\left[\hat\theta_\mathrm{IS}\right] = \frac{1}{n^2}\sum_{i=1}^n\widehat{\Var}\left[h(X_i)w(X_i)\right]= \frac{1}{n}\widehat{\Var}[h(X)w(X)]=\frac{1}{n(n-1)}\sum_{i=1}^n\left(h(X_i)w(X_i) - \hat\theta_\mathrm{IS}\right)^2.
$$
```{r Importance Sampling}
set.seed(321)

sample_from_proposal <- function(n) {
  u <- runif(n)
  sqrt(4^2 - 2 * log(1 - u))
}

n <- 100000
x <- sample_from_proposal(n)

w <- function(x) {
  f <- dnorm(x)                              # target density
  g <- ifelse(                               # proposal density
         test = x > 4,
         yes  = x * exp(-0.5 * (x^2 - 16)),
         no   = 0
       )
  return(f / g)
}

hw <- h(x) * w(x)

theta_IS <- sum(hw) / n  # Importance sampling estimate of Pr(X > 4)

sample_var_IS <- sum((hw - theta_IS)^2) / (n - 1)  # Sampling variance

t <- qt(0.05/2, df = n - 1, lower.tail = FALSE)  # quantile with 5% significance level
ci_IS <- theta_IS + t * sqrt(sample_var_IS / n) * c(-1, 1)  # Confidence Interval

# Result
list(
  theta_IS      = theta_IS,
  sample_var_IS = sample_var_IS,
  confint       = ci_IS,
  error         = abs(theta_IS - theta)
)
```

The number of samples $m$ needed for the simple Monte Carlo estimator to achieve the same precision as the importance sampling approach, we would need

$$
  m = n\frac{\widehat{\Var}[h(X)]}{\widehat{\Var}[h(X)w(X)]} = `r n`\frac{`r sample_var_MC`}{`r sample_var_IS`} = `r n * sample_var_MC / sample_var_IS`,
$$
samples. That is, we need about $10$ million times more samples.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.

### (a)
We modify `sample_from_proposal()` to return a pair of samples, where one takes $U\sim\Uniform(0,1)$ as argument and the other $1 - U$ as argument.

```{r Proposal function modified}
sample_from_proposal_mod <- function(n) {
  u <- runif(n)
  list(
    x_1 = sqrt(4^2 - 2 * log(1 - u)),
    x_2 = sqrt(4^2 - 2 * log(u))
  )
}
```


### (b)




[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem D: Rejection sampling and importance sampling

## 1.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 2.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 3.


[//]: # ----------------------------------------------------------------------------------------------------------------

## 4.


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------